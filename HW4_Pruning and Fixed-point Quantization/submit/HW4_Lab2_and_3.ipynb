{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab2 (a) Model preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from resnet20 import ResNetCIFAR\n",
    "from train_util import train, finetune, test\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from FP_layers import *\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net = ResNetCIFAR(num_layers=20, Nbits=None)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Test Loss=0.3231, Test accuracy=0.9151\n"
     ]
    }
   ],
   "source": [
    "# Load the best weight paramters\n",
    "net.load_state_dict(torch.load(\"pretrained_model.pt\"))\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab2 (b) Prune by percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_by_percentage(layer, q=70.0):\n",
    "    \"\"\"\n",
    "    Pruning the weight paramters by threshold.\n",
    "    :param q: pruning percentile. 'q' percent of the least \n",
    "    significant weight parameters will be pruned.\n",
    "    \"\"\"\n",
    "    # Convert the weight of \"layer\" to numpy array\n",
    "    weight = layer.weight.data.cpu().numpy()\n",
    "    \n",
    "    # Compute the q-th percentile of the abs of the converted array\n",
    "    threshold = np.percentile(np.abs(weight), q)\n",
    "    \n",
    "    # Generate a binary mask same shape as weight to decide which element to prune\n",
    "    mask = np.abs(weight) > threshold      # return True or False\n",
    "    \n",
    "    # Convert mask to torch tensor and put on GPU\n",
    "    mask = torch.from_numpy(mask).to(device)\n",
    "\n",
    "    # Multiply the weight by mask to perform pruning\n",
    "    layer.weight.data.mul_(mask)\n",
    "    \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of head_conv.0.conv: 0.6990740740740741\n",
      "Sparsity of body_op.0.conv1.0.conv: 0.7000868055555556\n",
      "Sparsity of body_op.0.conv2.0.conv: 0.7000868055555556\n",
      "Sparsity of body_op.1.conv1.0.conv: 0.7000868055555556\n",
      "Sparsity of body_op.1.conv2.0.conv: 0.7000868055555556\n",
      "Sparsity of body_op.2.conv1.0.conv: 0.7000868055555556\n",
      "Sparsity of body_op.2.conv2.0.conv: 0.7000868055555556\n",
      "Sparsity of body_op.3.conv1.0.conv: 0.6998697916666666\n",
      "Sparsity of body_op.3.conv2.0.conv: 0.6999782986111112\n",
      "Sparsity of body_op.4.conv1.0.conv: 0.6999782986111112\n",
      "Sparsity of body_op.4.conv2.0.conv: 0.6999782986111112\n",
      "Sparsity of body_op.5.conv1.0.conv: 0.6999782986111112\n",
      "Sparsity of body_op.5.conv2.0.conv: 0.6999782986111112\n",
      "Sparsity of body_op.6.conv1.0.conv: 0.6999782986111112\n",
      "Sparsity of body_op.6.conv2.0.conv: 0.7000054253472222\n",
      "Sparsity of body_op.7.conv1.0.conv: 0.7000054253472222\n",
      "Sparsity of body_op.7.conv2.0.conv: 0.7000054253472222\n",
      "Sparsity of body_op.8.conv1.0.conv: 0.7000054253472222\n",
      "Sparsity of body_op.8.conv2.0.conv: 0.7000054253472222\n",
      "Sparsity of final_fc.linear: 0.7\n",
      "Files already downloaded and verified\n",
      "Test Loss=2.4417, Test accuracy=0.4204\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"pretrained_model.pt\"))\n",
    "\n",
    "for name,layer in net.named_modules():\n",
    "    if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n",
    "        # change q value\n",
    "        prune_by_percentage(layer, q=70.0)\n",
    "        \n",
    "        # Optional: Check the sparsity you achieve in each layer\n",
    "        # Convert the weight of \"layer\" to numpy array\n",
    "        np_weight = layer.weight.data.cpu().numpy()\n",
    "        # Count number of zeros\n",
    "        zeros = np.sum(np_weight==0)\n",
    "        # Count number of parameters\n",
    "        total = np_weight.size\n",
    "        # Print sparsity\n",
    "        print('Sparsity of '+name+': '+str(zeros/total))\n",
    "        \n",
    "test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current q: 30\n",
      "Sparsity of head_conv.0.conv: 0.30092592592592593\n",
      "Sparsity of body_op.0.conv1.0.conv: 0.2999131944444444\n",
      "Sparsity of body_op.0.conv2.0.conv: 0.2999131944444444\n",
      "Sparsity of body_op.1.conv1.0.conv: 0.2999131944444444\n",
      "Sparsity of body_op.1.conv2.0.conv: 0.2999131944444444\n",
      "Sparsity of body_op.2.conv1.0.conv: 0.2999131944444444\n",
      "Sparsity of body_op.2.conv2.0.conv: 0.2999131944444444\n",
      "Sparsity of body_op.3.conv1.0.conv: 0.3001302083333333\n",
      "Sparsity of body_op.3.conv2.0.conv: 0.3000217013888889\n",
      "Sparsity of body_op.4.conv1.0.conv: 0.3000217013888889\n",
      "Sparsity of body_op.4.conv2.0.conv: 0.3000217013888889\n",
      "Sparsity of body_op.5.conv1.0.conv: 0.3000217013888889\n",
      "Sparsity of body_op.5.conv2.0.conv: 0.3000217013888889\n",
      "Sparsity of body_op.6.conv1.0.conv: 0.3000217013888889\n",
      "Sparsity of body_op.6.conv2.0.conv: 0.2999945746527778\n",
      "Sparsity of body_op.7.conv1.0.conv: 0.2999945746527778\n",
      "Sparsity of body_op.7.conv2.0.conv: 0.2999945746527778\n",
      "Sparsity of body_op.8.conv1.0.conv: 0.2999945746527778\n",
      "Sparsity of body_op.8.conv2.0.conv: 0.2999945746527778\n",
      "Sparsity of final_fc.linear: 0.3\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.3698, Test accuracy=0.9028\n",
      "========================\n",
      "Current q: 50\n",
      "Sparsity of head_conv.0.conv: 0.5\n",
      "Sparsity of body_op.0.conv1.0.conv: 0.5\n",
      "Sparsity of body_op.0.conv2.0.conv: 0.5\n",
      "Sparsity of body_op.1.conv1.0.conv: 0.5\n",
      "Sparsity of body_op.1.conv2.0.conv: 0.5\n",
      "Sparsity of body_op.2.conv1.0.conv: 0.5\n",
      "Sparsity of body_op.2.conv2.0.conv: 0.5\n",
      "Sparsity of body_op.3.conv1.0.conv: 0.5\n",
      "Sparsity of body_op.3.conv2.0.conv: 0.5\n",
      "Sparsity of body_op.4.conv1.0.conv: 0.5\n",
      "Sparsity of body_op.4.conv2.0.conv: 0.5\n",
      "Sparsity of body_op.5.conv1.0.conv: 0.5\n",
      "Sparsity of body_op.5.conv2.0.conv: 0.5\n",
      "Sparsity of body_op.6.conv1.0.conv: 0.5\n",
      "Sparsity of body_op.6.conv2.0.conv: 0.5\n",
      "Sparsity of body_op.7.conv1.0.conv: 0.5\n",
      "Sparsity of body_op.7.conv2.0.conv: 0.5\n",
      "Sparsity of body_op.8.conv1.0.conv: 0.5\n",
      "Sparsity of body_op.8.conv2.0.conv: 0.5\n",
      "Sparsity of final_fc.linear: 0.5\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.6774, Test accuracy=0.8210\n",
      "========================\n",
      "Current q: 70\n",
      "Sparsity of head_conv.0.conv: 0.6990740740740741\n",
      "Sparsity of body_op.0.conv1.0.conv: 0.7000868055555556\n",
      "Sparsity of body_op.0.conv2.0.conv: 0.7000868055555556\n",
      "Sparsity of body_op.1.conv1.0.conv: 0.7000868055555556\n",
      "Sparsity of body_op.1.conv2.0.conv: 0.7000868055555556\n",
      "Sparsity of body_op.2.conv1.0.conv: 0.7000868055555556\n",
      "Sparsity of body_op.2.conv2.0.conv: 0.7000868055555556\n",
      "Sparsity of body_op.3.conv1.0.conv: 0.6998697916666666\n",
      "Sparsity of body_op.3.conv2.0.conv: 0.6999782986111112\n",
      "Sparsity of body_op.4.conv1.0.conv: 0.6999782986111112\n",
      "Sparsity of body_op.4.conv2.0.conv: 0.6999782986111112\n",
      "Sparsity of body_op.5.conv1.0.conv: 0.6999782986111112\n",
      "Sparsity of body_op.5.conv2.0.conv: 0.6999782986111112\n",
      "Sparsity of body_op.6.conv1.0.conv: 0.6999782986111112\n",
      "Sparsity of body_op.6.conv2.0.conv: 0.7000054253472222\n",
      "Sparsity of body_op.7.conv1.0.conv: 0.7000054253472222\n",
      "Sparsity of body_op.7.conv2.0.conv: 0.7000054253472222\n",
      "Sparsity of body_op.8.conv1.0.conv: 0.7000054253472222\n",
      "Sparsity of body_op.8.conv2.0.conv: 0.7000054253472222\n",
      "Sparsity of final_fc.linear: 0.7\n",
      "Files already downloaded and verified\n",
      "Test Loss=2.4417, Test accuracy=0.4204\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "# try prunning q=0.3,0.5,0.7\n",
    "for q in [30, 50, 70]:\n",
    "    net.load_state_dict(torch.load(\"pretrained_model.pt\"))\n",
    "    print(f\"Current q: {q}\")\n",
    "\n",
    "    for name,layer in net.named_modules():\n",
    "        if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n",
    "            # change q value\n",
    "            prune_by_percentage(layer, q=q)\n",
    "            \n",
    "            # Optional: Check the sparsity you achieve in each layer\n",
    "            # Convert the weight of \"layer\" to numpy array\n",
    "            np_weight = layer.weight.data.cpu().numpy()\n",
    "            # Count number of zeros\n",
    "            zeros = np.sum(np_weight==0)\n",
    "            # Count number of parameters\n",
    "            total = np_weight.size\n",
    "            # Print sparsity\n",
    "            print('Sparsity of '+name+': '+str(zeros/total))\n",
    "            \n",
    "    test(net)\n",
    "    print('========================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab2 (c) Finetune pruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_after_prune(net, trainloader, criterion, optimizer, prune=True):\n",
    "    \"\"\"\n",
    "    Finetune the pruned model for a single epoch\n",
    "    Make sure pruned weights are kept as zero\n",
    "    \"\"\"\n",
    "    # Build a dictionary for the nonzero weights\n",
    "    weight_mask = {}\n",
    "    for name,layer in net.named_modules():\n",
    "        if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n",
    "            # Your code here: generate a mask in GPU torch tensor to have 1 for nonzero element and 0 for zero element \n",
    "            weight_mask[name] = (layer.weight.data != 0).float().to(device)\n",
    "    \n",
    "    global_steps = 0\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start = time.time()\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if prune:\n",
    "            for name,layer in net.named_modules():\n",
    "                if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n",
    "                    # Your code here: Use weight_mask to make sure zero elements remains zero\n",
    "                    layer.weight.data.mul_(weight_mask[name])\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        global_steps += 1\n",
    "\n",
    "        if global_steps % 50 == 0:\n",
    "            end = time.time()\n",
    "            batch_size = 256\n",
    "            num_examples_per_second = 50 * batch_size / (end - start)\n",
    "            print(\"[Step=%d]\\tLoss=%.4f\\tacc=%.4f\\t%.1f examples/second\"\n",
    "                 % (global_steps, train_loss / (batch_idx + 1), (correct / total), num_examples_per_second))\n",
    "            start = time.time()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Get pruned model\n",
    "net.load_state_dict(torch.load(\"pretrained_model.pt\"))\n",
    "for name,layer in net.named_modules():\n",
    "    if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n",
    "        prune_by_percentage(layer, q=70.0)\n",
    "\n",
    "# Training setup, do not change\n",
    "batch_size=256\n",
    "lr=0.002\n",
    "reg=1e-4\n",
    "\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.875, weight_decay=reg, nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "[Step=50]\tLoss=0.4065\tacc=0.8607\t3382.3 examples/second\n",
      "[Step=100]\tLoss=0.3671\tacc=0.8732\t7989.3 examples/second\n",
      "[Step=150]\tLoss=0.3392\tacc=0.8821\t8220.1 examples/second\n",
      "Test Loss=0.4290, Test acc=0.8663\n",
      "Saving...\n",
      "\n",
      "Epoch: 1\n",
      "[Step=50]\tLoss=0.2516\tacc=0.9130\t6868.3 examples/second\n",
      "[Step=100]\tLoss=0.2454\tacc=0.9143\t8057.2 examples/second\n",
      "[Step=150]\tLoss=0.2402\tacc=0.9160\t7841.6 examples/second\n",
      "Test Loss=0.3939, Test acc=0.8751\n",
      "Saving...\n",
      "\n",
      "Epoch: 2\n",
      "[Step=50]\tLoss=0.2096\tacc=0.9280\t6865.6 examples/second\n",
      "[Step=100]\tLoss=0.2096\tacc=0.9279\t8172.0 examples/second\n",
      "[Step=150]\tLoss=0.2085\tacc=0.9281\t7936.5 examples/second\n",
      "Test Loss=0.3777, Test acc=0.8794\n",
      "Saving...\n",
      "\n",
      "Epoch: 3\n",
      "[Step=50]\tLoss=0.2033\tacc=0.9293\t6514.7 examples/second\n",
      "[Step=100]\tLoss=0.2022\tacc=0.9285\t7991.9 examples/second\n",
      "[Step=150]\tLoss=0.1998\tacc=0.9298\t7826.6 examples/second\n",
      "Test Loss=0.3677, Test acc=0.8820\n",
      "Saving...\n",
      "\n",
      "Epoch: 4\n",
      "[Step=50]\tLoss=0.1877\tacc=0.9338\t6819.5 examples/second\n",
      "[Step=100]\tLoss=0.1860\tacc=0.9345\t8097.7 examples/second\n",
      "[Step=150]\tLoss=0.1883\tacc=0.9344\t7877.7 examples/second\n",
      "Test Loss=0.3625, Test acc=0.8836\n",
      "Saving...\n",
      "\n",
      "Epoch: 5\n",
      "[Step=50]\tLoss=0.1845\tacc=0.9361\t6837.9 examples/second\n",
      "[Step=100]\tLoss=0.1815\tacc=0.9368\t7934.7 examples/second\n",
      "[Step=150]\tLoss=0.1830\tacc=0.9366\t8008.8 examples/second\n",
      "Test Loss=0.3567, Test acc=0.8851\n",
      "Saving...\n",
      "\n",
      "Epoch: 6\n",
      "[Step=50]\tLoss=0.1837\tacc=0.9352\t6776.8 examples/second\n",
      "[Step=100]\tLoss=0.1773\tacc=0.9376\t7576.5 examples/second\n",
      "[Step=150]\tLoss=0.1765\tacc=0.9390\t8064.4 examples/second\n",
      "Test Loss=0.3537, Test acc=0.8867\n",
      "Saving...\n",
      "\n",
      "Epoch: 7\n",
      "[Step=50]\tLoss=0.1695\tacc=0.9434\t6681.7 examples/second\n",
      "[Step=100]\tLoss=0.1705\tacc=0.9421\t7889.9 examples/second\n",
      "[Step=150]\tLoss=0.1690\tacc=0.9418\t7559.8 examples/second\n",
      "Test Loss=0.3494, Test acc=0.8885\n",
      "Saving...\n",
      "\n",
      "Epoch: 8\n",
      "[Step=50]\tLoss=0.1683\tacc=0.9426\t6533.0 examples/second\n",
      "[Step=100]\tLoss=0.1687\tacc=0.9427\t7598.2 examples/second\n",
      "[Step=150]\tLoss=0.1659\tacc=0.9439\t7701.0 examples/second\n",
      "Test Loss=0.3463, Test acc=0.8877\n",
      "\n",
      "Epoch: 9\n",
      "[Step=50]\tLoss=0.1655\tacc=0.9432\t6415.5 examples/second\n",
      "[Step=100]\tLoss=0.1646\tacc=0.9429\t7718.2 examples/second\n",
      "[Step=150]\tLoss=0.1631\tacc=0.9434\t7732.5 examples/second\n",
      "Test Loss=0.3452, Test acc=0.8893\n",
      "Saving...\n",
      "\n",
      "Epoch: 10\n",
      "[Step=50]\tLoss=0.1606\tacc=0.9438\t6425.7 examples/second\n",
      "[Step=100]\tLoss=0.1598\tacc=0.9439\t7539.1 examples/second\n",
      "[Step=150]\tLoss=0.1596\tacc=0.9441\t7474.4 examples/second\n",
      "Test Loss=0.3441, Test acc=0.8895\n",
      "Saving...\n",
      "\n",
      "Epoch: 11\n",
      "[Step=50]\tLoss=0.1599\tacc=0.9445\t6315.9 examples/second\n",
      "[Step=100]\tLoss=0.1602\tacc=0.9443\t7445.8 examples/second\n",
      "[Step=150]\tLoss=0.1571\tacc=0.9455\t7598.4 examples/second\n",
      "Test Loss=0.3411, Test acc=0.8908\n",
      "Saving...\n",
      "\n",
      "Epoch: 12\n",
      "[Step=50]\tLoss=0.1468\tacc=0.9452\t6817.0 examples/second\n",
      "[Step=100]\tLoss=0.1537\tacc=0.9438\t7701.8 examples/second\n",
      "[Step=150]\tLoss=0.1540\tacc=0.9440\t7741.3 examples/second\n",
      "Test Loss=0.3395, Test acc=0.8895\n",
      "\n",
      "Epoch: 13\n",
      "[Step=50]\tLoss=0.1607\tacc=0.9466\t6718.5 examples/second\n",
      "[Step=100]\tLoss=0.1557\tacc=0.9473\t7814.0 examples/second\n",
      "[Step=150]\tLoss=0.1534\tacc=0.9477\t7574.7 examples/second\n",
      "Test Loss=0.3384, Test acc=0.8899\n",
      "\n",
      "Epoch: 14\n",
      "[Step=50]\tLoss=0.1464\tacc=0.9503\t6351.7 examples/second\n",
      "[Step=100]\tLoss=0.1494\tacc=0.9485\t7771.5 examples/second\n",
      "[Step=150]\tLoss=0.1492\tacc=0.9486\t7715.7 examples/second\n",
      "Test Loss=0.3376, Test acc=0.8927\n",
      "Saving...\n",
      "\n",
      "Epoch: 15\n",
      "[Step=50]\tLoss=0.1468\tacc=0.9480\t6444.5 examples/second\n",
      "[Step=100]\tLoss=0.1527\tacc=0.9455\t8047.5 examples/second\n",
      "[Step=150]\tLoss=0.1514\tacc=0.9466\t7744.0 examples/second\n",
      "Test Loss=0.3369, Test acc=0.8924\n",
      "\n",
      "Epoch: 16\n",
      "[Step=50]\tLoss=0.1495\tacc=0.9491\t6724.8 examples/second\n",
      "[Step=100]\tLoss=0.1488\tacc=0.9492\t7868.0 examples/second\n",
      "[Step=150]\tLoss=0.1486\tacc=0.9489\t7693.0 examples/second\n",
      "Test Loss=0.3360, Test acc=0.8929\n",
      "Saving...\n",
      "\n",
      "Epoch: 17\n",
      "[Step=50]\tLoss=0.1442\tacc=0.9523\t6784.8 examples/second\n",
      "[Step=100]\tLoss=0.1446\tacc=0.9514\t7677.0 examples/second\n",
      "[Step=150]\tLoss=0.1445\tacc=0.9516\t7487.9 examples/second\n",
      "Test Loss=0.3338, Test acc=0.8933\n",
      "Saving...\n",
      "\n",
      "Epoch: 18\n",
      "[Step=50]\tLoss=0.1394\tacc=0.9527\t6752.8 examples/second\n",
      "[Step=100]\tLoss=0.1421\tacc=0.9505\t7756.3 examples/second\n",
      "[Step=150]\tLoss=0.1445\tacc=0.9500\t7590.7 examples/second\n",
      "Test Loss=0.3346, Test acc=0.8926\n",
      "\n",
      "Epoch: 19\n",
      "[Step=50]\tLoss=0.1328\tacc=0.9542\t6301.2 examples/second\n",
      "[Step=100]\tLoss=0.1359\tacc=0.9525\t7873.9 examples/second\n",
      "[Step=150]\tLoss=0.1382\tacc=0.9520\t7576.4 examples/second\n",
      "Test Loss=0.3341, Test acc=0.8941\n",
      "Saving...\n"
     ]
    }
   ],
   "source": [
    "# Model finetuning\n",
    "for epoch in range(20):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    finetune_after_prune(net, trainloader, criterion, optimizer)\n",
    "    #Start the testing code.\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    num_val_steps = len(testloader)\n",
    "    val_acc = correct / total\n",
    "    print(\"Test Loss=%.4f, Test acc=%.4f\" % (test_loss / (num_val_steps), val_acc))\n",
    "\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        print(\"Saving...\")\n",
    "        torch.save(net.state_dict(), \"net_after_finetune.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_acc = [0.8663, 0.8751, 0.8794, 0.8820, 0.8836, 0.8851, 0.8867, 0.8885, 0.8877,  0.8893, 0.8895, 0.8908, 0.8895, 0.8899, 0.8927, 0.8924, 0.8929, 0.8933, 0.8926, 0.8941]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(test_acc)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.xticks(np.arange(0, 20, 5))\n",
    "plt.title('Test Accuracy vs Epoch')\n",
    "plt.text(11, 0.87, f'Best Test Accuracy: {max(test_acc):.4f}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of head_conv.0.conv: 0.6990740740740741\n",
      "Sparsity of body_op.0.conv1.0.conv: 0.7000868055555556\n",
      "Sparsity of body_op.0.conv2.0.conv: 0.7000868055555556\n",
      "Sparsity of body_op.1.conv1.0.conv: 0.7000868055555556\n",
      "Sparsity of body_op.1.conv2.0.conv: 0.7000868055555556\n",
      "Sparsity of body_op.2.conv1.0.conv: 0.7000868055555556\n",
      "Sparsity of body_op.2.conv2.0.conv: 0.7000868055555556\n",
      "Sparsity of body_op.3.conv1.0.conv: 0.6998697916666666\n",
      "Sparsity of body_op.3.conv2.0.conv: 0.6999782986111112\n",
      "Sparsity of body_op.4.conv1.0.conv: 0.6999782986111112\n",
      "Sparsity of body_op.4.conv2.0.conv: 0.6999782986111112\n",
      "Sparsity of body_op.5.conv1.0.conv: 0.6999782986111112\n",
      "Sparsity of body_op.5.conv2.0.conv: 0.6999782986111112\n",
      "Sparsity of body_op.6.conv1.0.conv: 0.6999782986111112\n",
      "Sparsity of body_op.6.conv2.0.conv: 0.7000054253472222\n",
      "Sparsity of body_op.7.conv1.0.conv: 0.7000054253472222\n",
      "Sparsity of body_op.7.conv2.0.conv: 0.7000054253472222\n",
      "Sparsity of body_op.8.conv1.0.conv: 0.7000054253472222\n",
      "Sparsity of body_op.8.conv2.0.conv: 0.7000054253472222\n",
      "Sparsity of final_fc.linear: 0.7\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.3341, Test accuracy=0.8941\n"
     ]
    }
   ],
   "source": [
    "# Check sparsity of the finetuned model, make sure it's not changed\n",
    "net.load_state_dict(torch.load(\"net_after_finetune.pt\"))\n",
    "\n",
    "for name,layer in net.named_modules():\n",
    "    if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n",
    "        # Your code here:\n",
    "        # Convert the weight of \"layer\" to numpy array\n",
    "        np_weight = layer.weight.data.cpu().numpy() \n",
    "        # Count number of zeros\n",
    "        zeros = np.sum(np_weight==0)\n",
    "        # Count number of parameters\n",
    "        total = np_weight.size\n",
    "        # Print sparsity\n",
    "        print('Sparsity of '+name+': '+str(zeros/total))\n",
    "\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab2 (d) Iterative pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "[Step=50]\tLoss=0.0463\tacc=0.9846\t6756.1 examples/second\n",
      "[Step=100]\tLoss=0.0491\tacc=0.9838\t8069.4 examples/second\n",
      "[Step=150]\tLoss=0.0485\tacc=0.9846\t7195.1 examples/second\n",
      "Test Loss=0.3263, Test acc=0.9152\n",
      "\n",
      "Epoch: 1\n",
      "[Step=50]\tLoss=0.0484\tacc=0.9843\t6859.3 examples/second\n",
      "[Step=100]\tLoss=0.0476\tacc=0.9846\t7950.5 examples/second\n",
      "[Step=150]\tLoss=0.0477\tacc=0.9846\t7195.2 examples/second\n",
      "Test Loss=0.3266, Test acc=0.9150\n",
      "\n",
      "Epoch: 2\n",
      "[Step=50]\tLoss=0.0471\tacc=0.9854\t6583.9 examples/second\n",
      "[Step=100]\tLoss=0.0498\tacc=0.9840\t7820.0 examples/second\n",
      "[Step=150]\tLoss=0.0513\tacc=0.9835\t7364.9 examples/second\n",
      "Test Loss=0.3273, Test acc=0.9140\n",
      "\n",
      "Epoch: 3\n",
      "[Step=50]\tLoss=0.0517\tacc=0.9840\t6243.6 examples/second\n",
      "[Step=100]\tLoss=0.0528\tacc=0.9832\t7845.9 examples/second\n",
      "[Step=150]\tLoss=0.0532\tacc=0.9832\t7498.1 examples/second\n",
      "Test Loss=0.3276, Test acc=0.9127\n",
      "\n",
      "Epoch: 4\n",
      "[Step=50]\tLoss=0.0617\tacc=0.9802\t6421.9 examples/second\n",
      "[Step=100]\tLoss=0.0622\tacc=0.9791\t7639.5 examples/second\n",
      "[Step=150]\tLoss=0.0605\tacc=0.9799\t7564.3 examples/second\n",
      "Test Loss=0.3323, Test acc=0.9115\n",
      "\n",
      "Epoch: 5\n",
      "[Step=50]\tLoss=0.0725\tacc=0.9755\t6599.0 examples/second\n",
      "[Step=100]\tLoss=0.0706\tacc=0.9761\t7998.6 examples/second\n",
      "[Step=150]\tLoss=0.0708\tacc=0.9756\t7506.5 examples/second\n",
      "Test Loss=0.3348, Test acc=0.9079\n",
      "\n",
      "Epoch: 6\n",
      "[Step=50]\tLoss=0.0891\tacc=0.9695\t6460.4 examples/second\n",
      "[Step=100]\tLoss=0.0878\tacc=0.9696\t7716.1 examples/second\n",
      "[Step=150]\tLoss=0.0872\tacc=0.9699\t7807.1 examples/second\n",
      "Test Loss=0.3330, Test acc=0.9058\n",
      "\n",
      "Epoch: 7\n",
      "[Step=50]\tLoss=0.1304\tacc=0.9540\t6745.9 examples/second\n",
      "[Step=100]\tLoss=0.1243\tacc=0.9558\t7119.3 examples/second\n",
      "[Step=150]\tLoss=0.1207\tacc=0.9576\t7846.1 examples/second\n",
      "Test Loss=0.3352, Test acc=0.9021\n",
      "\n",
      "Epoch: 8\n",
      "[Step=50]\tLoss=0.1701\tacc=0.9403\t6569.2 examples/second\n",
      "[Step=100]\tLoss=0.1633\tacc=0.9425\t7789.6 examples/second\n",
      "[Step=150]\tLoss=0.1551\tacc=0.9456\t7679.0 examples/second\n",
      "Test Loss=0.3446, Test acc=0.8940\n",
      "\n",
      "Epoch: 9\n",
      "[Step=50]\tLoss=0.2559\tacc=0.9098\t6649.4 examples/second\n",
      "[Step=100]\tLoss=0.2505\tacc=0.9123\t7622.4 examples/second\n",
      "[Step=150]\tLoss=0.2459\tacc=0.9144\t7721.4 examples/second\n",
      "Test Loss=0.3870, Test acc=0.8763\n",
      "\n",
      "Epoch: 10\n",
      "[Step=50]\tLoss=0.2129\tacc=0.9260\t6653.4 examples/second\n",
      "[Step=100]\tLoss=0.2058\tacc=0.9295\t7791.9 examples/second\n",
      "[Step=150]\tLoss=0.2034\tacc=0.9298\t7515.5 examples/second\n",
      "Test Loss=0.3729, Test acc=0.8801\n",
      "Saving...\n",
      "\n",
      "Epoch: 11\n",
      "[Step=50]\tLoss=0.1911\tacc=0.9343\t6265.7 examples/second\n",
      "[Step=100]\tLoss=0.1890\tacc=0.9343\t7706.2 examples/second\n",
      "[Step=150]\tLoss=0.1895\tacc=0.9345\t7253.9 examples/second\n",
      "Test Loss=0.3651, Test acc=0.8824\n",
      "Saving...\n",
      "\n",
      "Epoch: 12\n",
      "[Step=50]\tLoss=0.1902\tacc=0.9325\t6688.5 examples/second\n",
      "[Step=100]\tLoss=0.1863\tacc=0.9345\t8077.5 examples/second\n",
      "[Step=150]\tLoss=0.1844\tacc=0.9351\t7784.7 examples/second\n",
      "Test Loss=0.3573, Test acc=0.8834\n",
      "Saving...\n",
      "\n",
      "Epoch: 13\n",
      "[Step=50]\tLoss=0.1808\tacc=0.9362\t6067.2 examples/second\n",
      "[Step=100]\tLoss=0.1759\tacc=0.9373\t7887.7 examples/second\n",
      "[Step=150]\tLoss=0.1760\tacc=0.9370\t7344.2 examples/second\n",
      "Test Loss=0.3508, Test acc=0.8854\n",
      "Saving...\n",
      "\n",
      "Epoch: 14\n",
      "[Step=50]\tLoss=0.1629\tacc=0.9425\t6780.4 examples/second\n",
      "[Step=100]\tLoss=0.1676\tacc=0.9407\t7934.0 examples/second\n",
      "[Step=150]\tLoss=0.1675\tacc=0.9407\t7698.7 examples/second\n",
      "Test Loss=0.3473, Test acc=0.8870\n",
      "Saving...\n",
      "\n",
      "Epoch: 15\n",
      "[Step=50]\tLoss=0.1646\tacc=0.9434\t6125.3 examples/second\n",
      "[Step=100]\tLoss=0.1605\tacc=0.9451\t7698.3 examples/second\n",
      "[Step=150]\tLoss=0.1628\tacc=0.9439\t7630.6 examples/second\n",
      "Test Loss=0.3452, Test acc=0.8885\n",
      "Saving...\n",
      "\n",
      "Epoch: 16\n",
      "[Step=50]\tLoss=0.1560\tacc=0.9441\t6610.2 examples/second\n",
      "[Step=100]\tLoss=0.1563\tacc=0.9454\t8008.1 examples/second\n",
      "[Step=150]\tLoss=0.1609\tacc=0.9441\t7888.9 examples/second\n",
      "Test Loss=0.3417, Test acc=0.8899\n",
      "Saving...\n",
      "\n",
      "Epoch: 17\n",
      "[Step=50]\tLoss=0.1557\tacc=0.9447\t6699.5 examples/second\n",
      "[Step=100]\tLoss=0.1595\tacc=0.9439\t7843.4 examples/second\n",
      "[Step=150]\tLoss=0.1593\tacc=0.9443\t7435.1 examples/second\n",
      "Test Loss=0.3398, Test acc=0.8911\n",
      "Saving...\n",
      "\n",
      "Epoch: 18\n",
      "[Step=50]\tLoss=0.1557\tacc=0.9440\t6441.1 examples/second\n",
      "[Step=100]\tLoss=0.1555\tacc=0.9450\t7835.6 examples/second\n",
      "[Step=150]\tLoss=0.1529\tacc=0.9464\t7844.7 examples/second\n",
      "Test Loss=0.3386, Test acc=0.8901\n",
      "\n",
      "Epoch: 19\n",
      "[Step=50]\tLoss=0.1421\tacc=0.9521\t6420.2 examples/second\n",
      "[Step=100]\tLoss=0.1463\tacc=0.9505\t7786.5 examples/second\n",
      "[Step=150]\tLoss=0.1478\tacc=0.9493\t7802.1 examples/second\n",
      "Test Loss=0.3368, Test acc=0.8921\n",
      "Saving...\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"pretrained_model.pt\"))\n",
    "best_acc = 0.\n",
    "l_val_acc = []\n",
    "for epoch in range(20):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    \n",
    "    net.train()\n",
    "    if epoch<10:\n",
    "        for name,layer in net.named_modules():\n",
    "            if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n",
    "                # Increase model sparsity\n",
    "                q = 7 * (epoch+1)\n",
    "                prune_by_percentage(layer, q=q)\n",
    "    if epoch<9:\n",
    "        finetune_after_prune(net, trainloader, criterion, optimizer,prune=False)\n",
    "    else:\n",
    "        finetune_after_prune(net, trainloader, criterion, optimizer)\n",
    "    \n",
    "    #Start the testing code.\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    num_val_steps = len(testloader)\n",
    "    val_acc = correct / total\n",
    "    print(\"Test Loss=%.4f, Test acc=%.4f\" % (test_loss / (num_val_steps), val_acc))\n",
    "    l_val_acc.append(val_acc)\n",
    "    \n",
    "    if epoch>=10:\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            print(\"Saving...\")\n",
    "            torch.save(net.state_dict(), \"net_after_iterative_prune.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of head_conv.0.conv: 0.6990740740740741\n",
      "Sparsity of body_op.0.conv1.0.conv: 0.7000868055555556\n",
      "Sparsity of body_op.0.conv2.0.conv: 0.7000868055555556\n",
      "Sparsity of body_op.1.conv1.0.conv: 0.7000868055555556\n",
      "Sparsity of body_op.1.conv2.0.conv: 0.7000868055555556\n",
      "Sparsity of body_op.2.conv1.0.conv: 0.7000868055555556\n",
      "Sparsity of body_op.2.conv2.0.conv: 0.7000868055555556\n",
      "Sparsity of body_op.3.conv1.0.conv: 0.6998697916666666\n",
      "Sparsity of body_op.3.conv2.0.conv: 0.6999782986111112\n",
      "Sparsity of body_op.4.conv1.0.conv: 0.6999782986111112\n",
      "Sparsity of body_op.4.conv2.0.conv: 0.6999782986111112\n",
      "Sparsity of body_op.5.conv1.0.conv: 0.6999782986111112\n",
      "Sparsity of body_op.5.conv2.0.conv: 0.6999782986111112\n",
      "Sparsity of body_op.6.conv1.0.conv: 0.6999782986111112\n",
      "Sparsity of body_op.6.conv2.0.conv: 0.7000054253472222\n",
      "Sparsity of body_op.7.conv1.0.conv: 0.7000054253472222\n",
      "Sparsity of body_op.7.conv2.0.conv: 0.7000054253472222\n",
      "Sparsity of body_op.8.conv1.0.conv: 0.7000054253472222\n",
      "Sparsity of body_op.8.conv2.0.conv: 0.7000054253472222\n",
      "Sparsity of final_fc.linear: 0.7\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.3368, Test accuracy=0.8921\n"
     ]
    }
   ],
   "source": [
    "# Check sparsity of the final model, make sure it's 70%\n",
    "net.load_state_dict(torch.load(\"net_after_iterative_prune.pt\"))\n",
    "\n",
    "for name,layer in net.named_modules():\n",
    "    if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n",
    "        # Your code here: can copy from previous question\n",
    "        # Convert the weight of \"layer\" to numpy array\n",
    "        np_weight = layer.weight.data.cpu().numpy() \n",
    "        # Count number of zeros\n",
    "        zeros = np.sum(np_weight==0)\n",
    "        # Count number of parameters\n",
    "        total = np_weight.size\n",
    "        # Print sparsity\n",
    "        print('Sparsity of '+name+': '+str(zeros/total))        \n",
    "        \n",
    "test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the test accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(l_val_acc)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.xticks(np.arange(0, 20, 5))\n",
    "plt.title('Test Accuracy vs Epoch after iterative pruning')\n",
    "plt.text(14, 0.91, f'Best Test Accuracy: {max(l_val_acc):.4f}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab2 (e) Global iterative pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_prune_by_percentage(net, q=70.0):\n",
    "    \"\"\"\n",
    "    Pruning the weight paramters by threshold.\n",
    "    :param q: pruning percentile. 'q' percent of the least \n",
    "    significant weight parameters will be pruned.\n",
    "    \"\"\"\n",
    "    # A list to gather all the weights\n",
    "    flattened_weights = []\n",
    "    # Find global pruning threshold\n",
    "    for name,layer in net.named_modules():\n",
    "        if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n",
    "            # Convert weight to numpy\n",
    "            np_weight = layer.weight.data.cpu().numpy()\n",
    "            # Flatten the weight and append to flattened_weights\n",
    "            flattened_weights.append(np_weight.flatten())\n",
    "    \n",
    "    # Concate all weights into a np array\n",
    "    flattened_weights = np.concatenate(flattened_weights)\n",
    "    # Find global pruning threshold\n",
    "    thres = np.percentile(np.abs(flattened_weights), q)\n",
    "    \n",
    "    # Apply pruning threshold to all layers\n",
    "    for name,layer in net.named_modules():\n",
    "        if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n",
    "            # Convert weight to numpy\n",
    "            np_weight = layer.weight.data.cpu().numpy()\n",
    "            \n",
    "            # Generate a binary mask same shape as weight to decide which element to prune\n",
    "            mask = np.abs(np_weight) > thres\n",
    "            \n",
    "            # Convert mask to torch tensor and put on GPU\n",
    "            mask = torch.from_numpy(mask).to(device)\n",
    "            \n",
    "            # Multiply the weight by mask to perform pruning\n",
    "            layer.weight.data.mul_(mask)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "[Step=50]\tLoss=0.0490\tacc=0.9850\t6796.1 examples/second\n",
      "[Step=100]\tLoss=0.0492\tacc=0.9854\t7205.9 examples/second\n",
      "[Step=150]\tLoss=0.0492\tacc=0.9853\t7366.1 examples/second\n",
      "Test Loss=0.3262, Test acc=0.9141\n",
      "\n",
      "Epoch: 1\n",
      "[Step=50]\tLoss=0.0485\tacc=0.9839\t6674.1 examples/second\n",
      "[Step=100]\tLoss=0.0475\tacc=0.9848\t7548.9 examples/second\n",
      "[Step=150]\tLoss=0.0486\tacc=0.9841\t7798.1 examples/second\n",
      "Test Loss=0.3238, Test acc=0.9159\n",
      "\n",
      "Epoch: 2\n",
      "[Step=50]\tLoss=0.0477\tacc=0.9844\t6735.9 examples/second\n",
      "[Step=100]\tLoss=0.0489\tacc=0.9845\t8018.2 examples/second\n",
      "[Step=150]\tLoss=0.0492\tacc=0.9845\t7533.2 examples/second\n",
      "Test Loss=0.3239, Test acc=0.9139\n",
      "\n",
      "Epoch: 3\n",
      "[Step=50]\tLoss=0.0483\tacc=0.9844\t6656.8 examples/second\n",
      "[Step=100]\tLoss=0.0501\tacc=0.9836\t7977.2 examples/second\n",
      "[Step=150]\tLoss=0.0511\tacc=0.9834\t7870.8 examples/second\n",
      "Test Loss=0.3270, Test acc=0.9131\n",
      "\n",
      "Epoch: 4\n",
      "[Step=50]\tLoss=0.0556\tacc=0.9821\t6713.9 examples/second\n",
      "[Step=100]\tLoss=0.0557\tacc=0.9820\t7665.1 examples/second\n",
      "[Step=150]\tLoss=0.0550\tacc=0.9820\t7753.0 examples/second\n",
      "Test Loss=0.3242, Test acc=0.9129\n",
      "\n",
      "Epoch: 5\n",
      "[Step=50]\tLoss=0.0616\tacc=0.9801\t6578.1 examples/second\n",
      "[Step=100]\tLoss=0.0623\tacc=0.9797\t7985.9 examples/second\n",
      "[Step=150]\tLoss=0.0621\tacc=0.9797\t7493.9 examples/second\n",
      "Test Loss=0.3233, Test acc=0.9127\n",
      "\n",
      "Epoch: 6\n",
      "[Step=50]\tLoss=0.0744\tacc=0.9751\t6692.8 examples/second\n",
      "[Step=100]\tLoss=0.0729\tacc=0.9752\t7763.8 examples/second\n",
      "[Step=150]\tLoss=0.0727\tacc=0.9755\t7543.0 examples/second\n",
      "Test Loss=0.3257, Test acc=0.9084\n",
      "\n",
      "Epoch: 7\n",
      "[Step=50]\tLoss=0.0895\tacc=0.9685\t6678.3 examples/second\n",
      "[Step=100]\tLoss=0.0911\tacc=0.9677\t7611.4 examples/second\n",
      "[Step=150]\tLoss=0.0909\tacc=0.9678\t7743.5 examples/second\n",
      "Test Loss=0.3282, Test acc=0.9080\n",
      "\n",
      "Epoch: 8\n",
      "[Step=50]\tLoss=0.1223\tacc=0.9583\t6408.1 examples/second\n",
      "[Step=100]\tLoss=0.1209\tacc=0.9592\t7653.5 examples/second\n",
      "[Step=150]\tLoss=0.1192\tacc=0.9595\t7346.8 examples/second\n",
      "Test Loss=0.3273, Test acc=0.9015\n",
      "\n",
      "Epoch: 9\n",
      "[Step=50]\tLoss=0.1780\tacc=0.9395\t6525.6 examples/second\n",
      "[Step=100]\tLoss=0.1740\tacc=0.9412\t7420.4 examples/second\n",
      "[Step=150]\tLoss=0.1737\tacc=0.9409\t7803.5 examples/second\n",
      "Test Loss=0.3414, Test acc=0.8916\n",
      "\n",
      "Epoch: 10\n",
      "[Step=50]\tLoss=0.1610\tacc=0.9441\t6271.4 examples/second\n",
      "[Step=100]\tLoss=0.1556\tacc=0.9457\t7916.5 examples/second\n",
      "[Step=150]\tLoss=0.1540\tacc=0.9476\t7735.6 examples/second\n",
      "Test Loss=0.3369, Test acc=0.8916\n",
      "Saving...\n",
      "\n",
      "Epoch: 11\n",
      "[Step=50]\tLoss=0.1523\tacc=0.9459\t6104.0 examples/second\n",
      "[Step=100]\tLoss=0.1488\tacc=0.9486\t7444.4 examples/second\n",
      "[Step=150]\tLoss=0.1504\tacc=0.9486\t7622.3 examples/second\n",
      "Test Loss=0.3314, Test acc=0.8932\n",
      "Saving...\n",
      "\n",
      "Epoch: 12\n",
      "[Step=50]\tLoss=0.1440\tacc=0.9492\t6244.3 examples/second\n",
      "[Step=100]\tLoss=0.1450\tacc=0.9490\t7542.4 examples/second\n",
      "[Step=150]\tLoss=0.1445\tacc=0.9498\t7601.5 examples/second\n",
      "Test Loss=0.3275, Test acc=0.8938\n",
      "Saving...\n",
      "\n",
      "Epoch: 13\n",
      "[Step=50]\tLoss=0.1382\tacc=0.9547\t6599.4 examples/second\n",
      "[Step=100]\tLoss=0.1403\tacc=0.9524\t7637.7 examples/second\n",
      "[Step=150]\tLoss=0.1417\tacc=0.9522\t7648.5 examples/second\n",
      "Test Loss=0.3254, Test acc=0.8959\n",
      "Saving...\n",
      "\n",
      "Epoch: 14\n",
      "[Step=50]\tLoss=0.1360\tacc=0.9537\t6491.2 examples/second\n",
      "[Step=100]\tLoss=0.1383\tacc=0.9520\t7779.1 examples/second\n",
      "[Step=150]\tLoss=0.1372\tacc=0.9522\t7531.6 examples/second\n",
      "Test Loss=0.3232, Test acc=0.8958\n",
      "\n",
      "Epoch: 15\n",
      "[Step=50]\tLoss=0.1393\tacc=0.9498\t6151.0 examples/second\n",
      "[Step=100]\tLoss=0.1381\tacc=0.9522\t7963.2 examples/second\n",
      "[Step=150]\tLoss=0.1389\tacc=0.9521\t7725.9 examples/second\n",
      "Test Loss=0.3218, Test acc=0.8962\n",
      "Saving...\n",
      "\n",
      "Epoch: 16\n",
      "[Step=50]\tLoss=0.1370\tacc=0.9534\t6669.0 examples/second\n",
      "[Step=100]\tLoss=0.1354\tacc=0.9534\t7870.2 examples/second\n",
      "[Step=150]\tLoss=0.1325\tacc=0.9536\t7916.9 examples/second\n",
      "Test Loss=0.3214, Test acc=0.8971\n",
      "Saving...\n",
      "\n",
      "Epoch: 17\n",
      "[Step=50]\tLoss=0.1353\tacc=0.9535\t6655.7 examples/second\n",
      "[Step=100]\tLoss=0.1313\tacc=0.9554\t7605.0 examples/second\n",
      "[Step=150]\tLoss=0.1325\tacc=0.9548\t8061.6 examples/second\n",
      "Test Loss=0.3172, Test acc=0.8978\n",
      "Saving...\n",
      "\n",
      "Epoch: 18\n",
      "[Step=50]\tLoss=0.1261\tacc=0.9600\t6756.0 examples/second\n",
      "[Step=100]\tLoss=0.1272\tacc=0.9587\t8087.5 examples/second\n",
      "[Step=150]\tLoss=0.1283\tacc=0.9575\t7857.1 examples/second\n",
      "Test Loss=0.3174, Test acc=0.8982\n",
      "Saving...\n",
      "\n",
      "Epoch: 19\n",
      "[Step=50]\tLoss=0.1208\tacc=0.9598\t6598.1 examples/second\n",
      "[Step=100]\tLoss=0.1238\tacc=0.9581\t7855.2 examples/second\n",
      "[Step=150]\tLoss=0.1245\tacc=0.9586\t7755.8 examples/second\n",
      "Test Loss=0.3161, Test acc=0.8984\n",
      "Saving...\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"pretrained_model.pt\"))\n",
    "best_acc = 0.\n",
    "for epoch in range(20):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    q=(epoch+1)*7\n",
    "    \n",
    "    net.train()\n",
    "    # Increase model sparsity\n",
    "    if epoch<10:\n",
    "        global_prune_by_percentage(net, q=q)\n",
    "    if epoch<9:\n",
    "        finetune_after_prune(net, trainloader, criterion, optimizer,prune=False)\n",
    "    else:\n",
    "        finetune_after_prune(net, trainloader, criterion, optimizer)\n",
    "    \n",
    "    #Start the testing code.\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    num_val_steps = len(testloader)\n",
    "    val_acc = correct / total\n",
    "    print(\"Test Loss=%.4f, Test acc=%.4f\" % (test_loss / (num_val_steps), val_acc))\n",
    "    \n",
    "    if epoch>=10:\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            print(\"Saving...\")\n",
    "            torch.save(net.state_dict(), \"net_after_global_iterative_prune.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of head_conv.0.conv: 0.24305555555555555\n",
      "Sparsity of body_op.0.conv1.0.conv: 0.5494791666666666\n",
      "Sparsity of body_op.0.conv2.0.conv: 0.5282118055555556\n",
      "Sparsity of body_op.1.conv1.0.conv: 0.5190972222222222\n",
      "Sparsity of body_op.1.conv2.0.conv: 0.5520833333333334\n",
      "Sparsity of body_op.2.conv1.0.conv: 0.5182291666666666\n",
      "Sparsity of body_op.2.conv2.0.conv: 0.5638020833333334\n",
      "Sparsity of body_op.3.conv1.0.conv: 0.5260416666666666\n",
      "Sparsity of body_op.3.conv2.0.conv: 0.5826822916666666\n",
      "Sparsity of body_op.4.conv1.0.conv: 0.6153428819444444\n",
      "Sparsity of body_op.4.conv2.0.conv: 0.6766493055555556\n",
      "Sparsity of body_op.5.conv1.0.conv: 0.611328125\n",
      "Sparsity of body_op.5.conv2.0.conv: 0.7038845486111112\n",
      "Sparsity of body_op.6.conv1.0.conv: 0.6153428819444444\n",
      "Sparsity of body_op.6.conv2.0.conv: 0.6513129340277778\n",
      "Sparsity of body_op.7.conv1.0.conv: 0.6624891493055556\n",
      "Sparsity of body_op.7.conv2.0.conv: 0.718994140625\n",
      "Sparsity of body_op.8.conv1.0.conv: 0.7477756076388888\n",
      "Sparsity of body_op.8.conv2.0.conv: 0.93701171875\n",
      "Sparsity of final_fc.linear: 0.1203125\n",
      "Total sparsity of: 0.6999992546657922\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.3161, Test accuracy=0.8984\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"net_after_global_iterative_prune.pt\"))\n",
    "\n",
    "zeros_sum = 0\n",
    "total_sum = 0\n",
    "for name,layer in net.named_modules():\n",
    "    if (isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)) and 'id_mapping' not in name:\n",
    "        # Your code here:\n",
    "        # Convert the weight of \"layer\" to numpy array\n",
    "        np_weight = layer.weight.data.cpu().numpy() \n",
    "        # Count number of zeros\n",
    "        zeros = np.sum(np_weight==0)\n",
    "        # Count number of parameters\n",
    "        total = np_weight.size\n",
    "        zeros_sum+=zeros\n",
    "        total_sum+=total\n",
    "        print('Sparsity of '+name+': '+str(zeros/total))\n",
    "print('Total sparsity of: '+str(zeros_sum/total_sum))\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 3 (b) and (c): Fixed-point quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Test Loss=0.3861, Test accuracy=0.8972\n"
     ]
    }
   ],
   "source": [
    "# Define quantized model and load weight\n",
    "Nbits = 4 #Change this value to finish (b) and (c)\n",
    "\n",
    "net = ResNetCIFAR(num_layers=20, Nbits=Nbits)\n",
    "net = net.to(device)\n",
    "net.load_state_dict(torch.load(\"pretrained_model.pt\"))\n",
    "test(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Epoch: 0\n",
      "[Step=50]\tLoss=0.0670\tacc=0.9768\t3843.1 examples/second\n",
      "[Step=100]\tLoss=0.0682\tacc=0.9764\t3904.7 examples/second\n",
      "[Step=150]\tLoss=0.0682\tacc=0.9764\t4027.0 examples/second\n",
      "Test Loss=0.3363, Test acc=0.9093\n",
      "Saving...\n",
      "\n",
      "Epoch: 1\n",
      "[Step=200]\tLoss=0.0515\tacc=0.9834\t2486.3 examples/second\n",
      "[Step=250]\tLoss=0.0633\tacc=0.9789\t3963.4 examples/second\n",
      "[Step=300]\tLoss=0.0603\tacc=0.9797\t4007.5 examples/second\n",
      "[Step=350]\tLoss=0.0605\tacc=0.9794\t4012.5 examples/second\n",
      "Test Loss=0.3340, Test acc=0.9111\n",
      "Saving...\n",
      "\n",
      "Epoch: 2\n",
      "[Step=400]\tLoss=0.0645\tacc=0.9805\t2453.2 examples/second\n",
      "[Step=450]\tLoss=0.0600\tacc=0.9795\t3989.8 examples/second\n",
      "[Step=500]\tLoss=0.0610\tacc=0.9795\t3956.3 examples/second\n",
      "[Step=550]\tLoss=0.0616\tacc=0.9795\t3984.6 examples/second\n",
      "Test Loss=0.3324, Test acc=0.9120\n",
      "Saving...\n",
      "\n",
      "Epoch: 3\n",
      "[Step=600]\tLoss=0.0598\tacc=0.9818\t2456.0 examples/second\n",
      "[Step=650]\tLoss=0.0600\tacc=0.9807\t3785.4 examples/second\n",
      "[Step=700]\tLoss=0.0588\tacc=0.9812\t4015.2 examples/second\n",
      "[Step=750]\tLoss=0.0601\tacc=0.9805\t4028.8 examples/second\n",
      "Test Loss=0.3318, Test acc=0.9123\n",
      "Saving...\n",
      "\n",
      "Epoch: 4\n",
      "[Step=800]\tLoss=0.0580\tacc=0.9780\t2442.3 examples/second\n",
      "[Step=850]\tLoss=0.0589\tacc=0.9798\t3748.9 examples/second\n",
      "[Step=900]\tLoss=0.0612\tacc=0.9790\t3848.4 examples/second\n",
      "[Step=950]\tLoss=0.0607\tacc=0.9789\t3873.2 examples/second\n",
      "Test Loss=0.3286, Test acc=0.9131\n",
      "Saving...\n",
      "\n",
      "Epoch: 5\n",
      "[Step=1000]\tLoss=0.0511\tacc=0.9834\t2438.4 examples/second\n",
      "[Step=1050]\tLoss=0.0570\tacc=0.9806\t3793.9 examples/second\n",
      "[Step=1100]\tLoss=0.0586\tacc=0.9805\t3711.9 examples/second\n",
      "[Step=1150]\tLoss=0.0576\tacc=0.9807\t3958.7 examples/second\n",
      "Test Loss=0.3322, Test acc=0.9131\n",
      "\n",
      "Epoch: 6\n",
      "[Step=1200]\tLoss=0.0574\tacc=0.9821\t2469.5 examples/second\n",
      "[Step=1250]\tLoss=0.0571\tacc=0.9815\t3908.1 examples/second\n",
      "[Step=1300]\tLoss=0.0580\tacc=0.9805\t3815.7 examples/second\n",
      "[Step=1350]\tLoss=0.0587\tacc=0.9806\t4031.5 examples/second\n",
      "Test Loss=0.3315, Test acc=0.9113\n",
      "\n",
      "Epoch: 7\n",
      "[Step=1400]\tLoss=0.0590\tacc=0.9802\t2478.8 examples/second\n",
      "[Step=1450]\tLoss=0.0556\tacc=0.9819\t3966.2 examples/second\n",
      "[Step=1500]\tLoss=0.0556\tacc=0.9816\t3981.3 examples/second\n",
      "[Step=1550]\tLoss=0.0556\tacc=0.9815\t3939.5 examples/second\n",
      "Test Loss=0.3310, Test acc=0.9136\n",
      "Saving...\n",
      "\n",
      "Epoch: 8\n",
      "[Step=1600]\tLoss=0.0538\tacc=0.9825\t2392.6 examples/second\n",
      "[Step=1650]\tLoss=0.0554\tacc=0.9815\t3856.6 examples/second\n",
      "[Step=1700]\tLoss=0.0570\tacc=0.9811\t3909.5 examples/second\n",
      "[Step=1750]\tLoss=0.0564\tacc=0.9814\t3886.9 examples/second\n",
      "Test Loss=0.3322, Test acc=0.9135\n",
      "\n",
      "Epoch: 9\n",
      "[Step=1800]\tLoss=0.0568\tacc=0.9813\t2483.2 examples/second\n",
      "[Step=1850]\tLoss=0.0563\tacc=0.9813\t4007.5 examples/second\n",
      "[Step=1900]\tLoss=0.0560\tacc=0.9815\t4005.6 examples/second\n",
      "[Step=1950]\tLoss=0.0563\tacc=0.9812\t3984.3 examples/second\n",
      "Test Loss=0.3326, Test acc=0.9135\n",
      "\n",
      "Epoch: 10\n",
      "[Step=2000]\tLoss=0.0536\tacc=0.9828\t2454.2 examples/second\n",
      "[Step=2050]\tLoss=0.0539\tacc=0.9826\t3964.0 examples/second\n",
      "[Step=2100]\tLoss=0.0537\tacc=0.9824\t3952.6 examples/second\n",
      "[Step=2150]\tLoss=0.0545\tacc=0.9822\t3976.0 examples/second\n",
      "Test Loss=0.3297, Test acc=0.9123\n",
      "\n",
      "Epoch: 11\n",
      "[Step=2200]\tLoss=0.0566\tacc=0.9808\t2436.2 examples/second\n",
      "[Step=2250]\tLoss=0.0551\tacc=0.9825\t4007.6 examples/second\n",
      "[Step=2300]\tLoss=0.0560\tacc=0.9818\t3981.5 examples/second\n",
      "[Step=2350]\tLoss=0.0572\tacc=0.9810\t4030.6 examples/second\n",
      "Test Loss=0.3332, Test acc=0.9113\n",
      "\n",
      "Epoch: 12\n",
      "[Step=2400]\tLoss=0.0586\tacc=0.9793\t2415.9 examples/second\n",
      "[Step=2450]\tLoss=0.0565\tacc=0.9808\t3806.3 examples/second\n",
      "[Step=2500]\tLoss=0.0574\tacc=0.9806\t3962.5 examples/second\n",
      "Test Loss=0.3313, Test acc=0.9121\n",
      "\n",
      "Epoch: 13\n",
      "[Step=2550]\tLoss=0.0417\tacc=0.9902\t2463.2 examples/second\n",
      "[Step=2600]\tLoss=0.0549\tacc=0.9807\t3894.0 examples/second\n",
      "[Step=2650]\tLoss=0.0552\tacc=0.9817\t3970.2 examples/second\n",
      "[Step=2700]\tLoss=0.0552\tacc=0.9821\t3868.7 examples/second\n",
      "Test Loss=0.3323, Test acc=0.9122\n",
      "\n",
      "Epoch: 14\n",
      "[Step=2750]\tLoss=0.0544\tacc=0.9792\t2437.5 examples/second\n",
      "[Step=2800]\tLoss=0.0514\tacc=0.9824\t3818.5 examples/second\n",
      "[Step=2850]\tLoss=0.0527\tacc=0.9822\t3872.2 examples/second\n",
      "[Step=2900]\tLoss=0.0543\tacc=0.9820\t3993.4 examples/second\n",
      "Test Loss=0.3326, Test acc=0.9117\n",
      "\n",
      "Epoch: 15\n",
      "[Step=2950]\tLoss=0.0543\tacc=0.9812\t2466.7 examples/second\n",
      "[Step=3000]\tLoss=0.0523\tacc=0.9816\t3936.9 examples/second\n",
      "[Step=3050]\tLoss=0.0531\tacc=0.9821\t3657.5 examples/second\n",
      "[Step=3100]\tLoss=0.0541\tacc=0.9821\t3867.7 examples/second\n",
      "Test Loss=0.3372, Test acc=0.9112\n",
      "\n",
      "Epoch: 16\n",
      "[Step=3150]\tLoss=0.0484\tacc=0.9849\t2494.5 examples/second\n",
      "[Step=3200]\tLoss=0.0554\tacc=0.9821\t4018.7 examples/second\n",
      "[Step=3250]\tLoss=0.0536\tacc=0.9825\t3972.0 examples/second\n",
      "[Step=3300]\tLoss=0.0534\tacc=0.9827\t3915.8 examples/second\n",
      "Test Loss=0.3386, Test acc=0.9131\n",
      "\n",
      "Epoch: 17\n",
      "[Step=3350]\tLoss=0.0550\tacc=0.9800\t2468.4 examples/second\n",
      "[Step=3400]\tLoss=0.0533\tacc=0.9818\t4012.8 examples/second\n",
      "[Step=3450]\tLoss=0.0513\tacc=0.9830\t3970.4 examples/second\n",
      "[Step=3500]\tLoss=0.0532\tacc=0.9820\t3977.9 examples/second\n",
      "Test Loss=0.3310, Test acc=0.9134\n",
      "\n",
      "Epoch: 18\n",
      "[Step=3550]\tLoss=0.0528\tacc=0.9838\t2450.3 examples/second\n",
      "[Step=3600]\tLoss=0.0525\tacc=0.9830\t3972.7 examples/second\n",
      "[Step=3650]\tLoss=0.0545\tacc=0.9826\t3998.2 examples/second\n",
      "[Step=3700]\tLoss=0.0545\tacc=0.9820\t3955.0 examples/second\n",
      "Test Loss=0.3335, Test acc=0.9120\n",
      "\n",
      "Epoch: 19\n",
      "[Step=3750]\tLoss=0.0528\tacc=0.9841\t2523.6 examples/second\n",
      "[Step=3800]\tLoss=0.0518\tacc=0.9840\t4046.5 examples/second\n",
      "[Step=3850]\tLoss=0.0520\tacc=0.9837\t4025.9 examples/second\n",
      "[Step=3900]\tLoss=0.0525\tacc=0.9831\t3894.6 examples/second\n",
      "Test Loss=0.3318, Test acc=0.9150\n",
      "Saving...\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.3318, Test accuracy=0.9150\n"
     ]
    }
   ],
   "source": [
    "# Quantized model finetuning\n",
    "finetune(net, epochs=20, batch_size=256, lr=0.002, reg=1e-4)   \n",
    "\n",
    "# Load the model with best accuracy\n",
    "net.load_state_dict(torch.load(\"quantized_net_after_finetune.pt\"))\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Nbits: 2\n",
      "Files already downloaded and verified\n",
      "Test Loss=9.5441, Test accuracy=0.0899\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Epoch: 0\n",
      "[Step=50]\tLoss=1.3515\tacc=0.6227\t3681.2 examples/second\n",
      "[Step=100]\tLoss=1.0904\tacc=0.6814\t3922.4 examples/second\n",
      "[Step=150]\tLoss=0.9601\tacc=0.7108\t4042.0 examples/second\n",
      "Test Loss=0.7263, Test acc=0.7803\n",
      "Saving...\n",
      "\n",
      "Epoch: 1\n",
      "[Step=200]\tLoss=0.5697\tacc=0.8066\t2452.9 examples/second\n",
      "[Step=250]\tLoss=0.5913\tacc=0.7993\t3936.6 examples/second\n",
      "[Step=300]\tLoss=0.5576\tacc=0.8101\t3946.3 examples/second\n",
      "[Step=350]\tLoss=0.5462\tacc=0.8132\t3962.0 examples/second\n",
      "Test Loss=0.6394, Test acc=0.8049\n",
      "Saving...\n",
      "\n",
      "Epoch: 2\n",
      "[Step=400]\tLoss=0.5368\tacc=0.8184\t2414.9 examples/second\n",
      "[Step=450]\tLoss=0.4848\tacc=0.8324\t3973.0 examples/second\n",
      "[Step=500]\tLoss=0.4696\tacc=0.8384\t3968.3 examples/second\n",
      "[Step=550]\tLoss=0.4653\tacc=0.8407\t4001.2 examples/second\n",
      "Test Loss=0.5700, Test acc=0.8201\n",
      "Saving...\n",
      "\n",
      "Epoch: 3\n",
      "[Step=600]\tLoss=0.4219\tacc=0.8558\t2375.9 examples/second\n",
      "[Step=650]\tLoss=0.4287\tacc=0.8528\t3952.6 examples/second\n",
      "[Step=700]\tLoss=0.4197\tacc=0.8563\t3927.4 examples/second\n",
      "[Step=750]\tLoss=0.4248\tacc=0.8543\t3842.6 examples/second\n",
      "Test Loss=0.5866, Test acc=0.8194\n",
      "\n",
      "Epoch: 4\n",
      "[Step=800]\tLoss=0.4164\tacc=0.8572\t2403.6 examples/second\n",
      "[Step=850]\tLoss=0.4182\tacc=0.8549\t3898.7 examples/second\n",
      "[Step=900]\tLoss=0.4075\tacc=0.8583\t3926.2 examples/second\n",
      "[Step=950]\tLoss=0.3968\tacc=0.8618\t3979.2 examples/second\n",
      "Test Loss=0.5665, Test acc=0.8242\n",
      "Saving...\n",
      "\n",
      "Epoch: 5\n",
      "[Step=1000]\tLoss=0.3682\tacc=0.8699\t2395.4 examples/second\n",
      "[Step=1050]\tLoss=0.3869\tacc=0.8658\t3946.0 examples/second\n",
      "[Step=1100]\tLoss=0.3771\tacc=0.8686\t3967.3 examples/second\n",
      "[Step=1150]\tLoss=0.3719\tacc=0.8701\t3931.0 examples/second\n",
      "Test Loss=0.6165, Test acc=0.8208\n",
      "\n",
      "Epoch: 6\n",
      "[Step=1200]\tLoss=0.3631\tacc=0.8703\t2385.6 examples/second\n",
      "[Step=1250]\tLoss=0.3670\tacc=0.8703\t3723.6 examples/second\n",
      "[Step=1300]\tLoss=0.3576\tacc=0.8738\t3779.6 examples/second\n",
      "[Step=1350]\tLoss=0.3583\tacc=0.8734\t3764.1 examples/second\n",
      "Test Loss=0.5153, Test acc=0.8421\n",
      "Saving...\n",
      "\n",
      "Epoch: 7\n",
      "[Step=1400]\tLoss=0.3444\tacc=0.8843\t2436.7 examples/second\n",
      "[Step=1450]\tLoss=0.3356\tacc=0.8840\t3874.2 examples/second\n",
      "[Step=1500]\tLoss=0.3410\tacc=0.8822\t3692.7 examples/second\n",
      "[Step=1550]\tLoss=0.3449\tacc=0.8809\t3923.6 examples/second\n",
      "Test Loss=0.5068, Test acc=0.8440\n",
      "Saving...\n",
      "\n",
      "Epoch: 8\n",
      "[Step=1600]\tLoss=0.3297\tacc=0.8861\t2352.2 examples/second\n",
      "[Step=1650]\tLoss=0.3381\tacc=0.8812\t3855.5 examples/second\n",
      "[Step=1700]\tLoss=0.3363\tacc=0.8823\t3962.1 examples/second\n",
      "[Step=1750]\tLoss=0.3374\tacc=0.8817\t3977.8 examples/second\n",
      "Test Loss=0.5605, Test acc=0.8275\n",
      "\n",
      "Epoch: 9\n",
      "[Step=1800]\tLoss=0.3386\tacc=0.8818\t2461.3 examples/second\n",
      "[Step=1850]\tLoss=0.3263\tacc=0.8843\t3933.5 examples/second\n",
      "[Step=1900]\tLoss=0.3323\tacc=0.8842\t3795.5 examples/second\n",
      "[Step=1950]\tLoss=0.3296\tacc=0.8851\t3813.7 examples/second\n",
      "Test Loss=0.4760, Test acc=0.8508\n",
      "Saving...\n",
      "\n",
      "Epoch: 10\n",
      "[Step=2000]\tLoss=0.3222\tacc=0.8880\t2463.3 examples/second\n",
      "[Step=2050]\tLoss=0.3267\tacc=0.8855\t3734.6 examples/second\n",
      "[Step=2100]\tLoss=0.3215\tacc=0.8875\t3969.1 examples/second\n",
      "[Step=2150]\tLoss=0.3220\tacc=0.8867\t3942.2 examples/second\n",
      "Test Loss=0.5066, Test acc=0.8462\n",
      "\n",
      "Epoch: 11\n",
      "[Step=2200]\tLoss=0.3216\tacc=0.8831\t2450.5 examples/second\n",
      "[Step=2250]\tLoss=0.3210\tacc=0.8873\t3963.2 examples/second\n",
      "[Step=2300]\tLoss=0.3178\tacc=0.8887\t3918.0 examples/second\n",
      "[Step=2350]\tLoss=0.3140\tacc=0.8902\t3661.4 examples/second\n",
      "Test Loss=0.4961, Test acc=0.8486\n",
      "\n",
      "Epoch: 12\n",
      "[Step=2400]\tLoss=0.3011\tacc=0.8962\t2412.3 examples/second\n",
      "[Step=2450]\tLoss=0.3112\tacc=0.8916\t3889.0 examples/second\n",
      "[Step=2500]\tLoss=0.3058\tacc=0.8927\t3955.6 examples/second\n",
      "Test Loss=0.4771, Test acc=0.8536\n",
      "Saving...\n",
      "\n",
      "Epoch: 13\n",
      "[Step=2550]\tLoss=0.3497\tacc=0.8672\t2423.3 examples/second\n",
      "[Step=2600]\tLoss=0.3108\tacc=0.8925\t3920.1 examples/second\n",
      "[Step=2650]\tLoss=0.3045\tacc=0.8933\t3871.9 examples/second\n",
      "[Step=2700]\tLoss=0.3081\tacc=0.8909\t3961.4 examples/second\n",
      "Test Loss=0.4700, Test acc=0.8529\n",
      "\n",
      "Epoch: 14\n",
      "[Step=2750]\tLoss=0.2795\tacc=0.8991\t2397.3 examples/second\n",
      "[Step=2800]\tLoss=0.3013\tacc=0.8954\t3643.1 examples/second\n",
      "[Step=2850]\tLoss=0.2957\tacc=0.8954\t3757.8 examples/second\n",
      "[Step=2900]\tLoss=0.2941\tacc=0.8965\t3658.7 examples/second\n",
      "Test Loss=0.4920, Test acc=0.8486\n",
      "\n",
      "Epoch: 15\n",
      "[Step=2950]\tLoss=0.3071\tacc=0.8895\t2448.5 examples/second\n",
      "[Step=3000]\tLoss=0.2952\tacc=0.8966\t3894.0 examples/second\n",
      "[Step=3050]\tLoss=0.2904\tacc=0.8975\t3946.7 examples/second\n",
      "[Step=3100]\tLoss=0.2869\tacc=0.8988\t3965.4 examples/second\n",
      "Test Loss=0.4741, Test acc=0.8545\n",
      "Saving...\n",
      "\n",
      "Epoch: 16\n",
      "[Step=3150]\tLoss=0.2979\tacc=0.8951\t2429.9 examples/second\n",
      "[Step=3200]\tLoss=0.2857\tacc=0.8987\t3966.4 examples/second\n",
      "[Step=3250]\tLoss=0.2907\tacc=0.8983\t3916.3 examples/second\n",
      "[Step=3300]\tLoss=0.2906\tacc=0.8981\t3616.2 examples/second\n",
      "Test Loss=0.4793, Test acc=0.8536\n",
      "\n",
      "Epoch: 17\n",
      "[Step=3350]\tLoss=0.2729\tacc=0.9036\t2444.0 examples/second\n",
      "[Step=3400]\tLoss=0.2871\tacc=0.8990\t3911.3 examples/second\n",
      "[Step=3450]\tLoss=0.2886\tacc=0.8989\t3945.1 examples/second\n",
      "[Step=3500]\tLoss=0.2885\tacc=0.8987\t3953.8 examples/second\n",
      "Test Loss=0.4962, Test acc=0.8504\n",
      "\n",
      "Epoch: 18\n",
      "[Step=3550]\tLoss=0.2923\tacc=0.8967\t2427.8 examples/second\n",
      "[Step=3600]\tLoss=0.2754\tacc=0.9011\t3967.4 examples/second\n",
      "[Step=3650]\tLoss=0.2717\tacc=0.9024\t3971.2 examples/second\n",
      "[Step=3700]\tLoss=0.2761\tacc=0.9018\t3785.7 examples/second\n",
      "Test Loss=0.4930, Test acc=0.8483\n",
      "\n",
      "Epoch: 19\n",
      "[Step=3750]\tLoss=0.2700\tacc=0.9019\t2459.0 examples/second\n",
      "[Step=3800]\tLoss=0.2852\tacc=0.8964\t3969.4 examples/second\n",
      "[Step=3850]\tLoss=0.2800\tacc=0.8981\t3966.3 examples/second\n",
      "[Step=3900]\tLoss=0.2788\tacc=0.9000\t3937.4 examples/second\n",
      "Test Loss=0.4654, Test acc=0.8550\n",
      "Saving...\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.4654, Test accuracy=0.8550\n",
      "========================\n",
      "Current Nbits: 3\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.9874, Test accuracy=0.7662\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Epoch: 0\n",
      "[Step=50]\tLoss=0.1755\tacc=0.9395\t3733.8 examples/second\n",
      "[Step=100]\tLoss=0.1631\tacc=0.9437\t3962.8 examples/second\n",
      "[Step=150]\tLoss=0.1537\tacc=0.9460\t3949.6 examples/second\n",
      "Test Loss=0.3912, Test acc=0.8964\n",
      "Saving...\n",
      "\n",
      "Epoch: 1\n",
      "[Step=200]\tLoss=0.1150\tacc=0.9619\t2446.3 examples/second\n",
      "[Step=250]\tLoss=0.1168\tacc=0.9577\t3840.7 examples/second\n",
      "[Step=300]\tLoss=0.1214\tacc=0.9560\t3720.8 examples/second\n",
      "[Step=350]\tLoss=0.1180\tacc=0.9574\t3929.4 examples/second\n",
      "Test Loss=0.3800, Test acc=0.8970\n",
      "Saving...\n",
      "\n",
      "Epoch: 2\n",
      "[Step=400]\tLoss=0.1226\tacc=0.9526\t2477.1 examples/second\n",
      "[Step=450]\tLoss=0.1171\tacc=0.9568\t3989.4 examples/second\n",
      "[Step=500]\tLoss=0.1126\tacc=0.9588\t3958.1 examples/second\n",
      "[Step=550]\tLoss=0.1115\tacc=0.9594\t3928.8 examples/second\n",
      "Test Loss=0.3759, Test acc=0.9007\n",
      "Saving...\n",
      "\n",
      "Epoch: 3\n",
      "[Step=600]\tLoss=0.1039\tacc=0.9658\t2461.6 examples/second\n",
      "[Step=650]\tLoss=0.1092\tacc=0.9610\t3909.5 examples/second\n",
      "[Step=700]\tLoss=0.1087\tacc=0.9613\t3972.8 examples/second\n",
      "[Step=750]\tLoss=0.1089\tacc=0.9611\t3945.0 examples/second\n",
      "Test Loss=0.3752, Test acc=0.8996\n",
      "\n",
      "Epoch: 4\n",
      "[Step=800]\tLoss=0.1127\tacc=0.9578\t2462.7 examples/second\n",
      "[Step=850]\tLoss=0.1058\tacc=0.9606\t3941.4 examples/second\n",
      "[Step=900]\tLoss=0.1069\tacc=0.9613\t3844.5 examples/second\n",
      "[Step=950]\tLoss=0.1060\tacc=0.9614\t3709.3 examples/second\n",
      "Test Loss=0.3666, Test acc=0.9009\n",
      "Saving...\n",
      "\n",
      "Epoch: 5\n",
      "[Step=1000]\tLoss=0.0947\tacc=0.9684\t2420.3 examples/second\n",
      "[Step=1050]\tLoss=0.1009\tacc=0.9655\t3962.9 examples/second\n",
      "[Step=1100]\tLoss=0.1004\tacc=0.9647\t3740.3 examples/second\n",
      "[Step=1150]\tLoss=0.1012\tacc=0.9640\t3916.2 examples/second\n",
      "Test Loss=0.3670, Test acc=0.9012\n",
      "Saving...\n",
      "\n",
      "Epoch: 6\n",
      "[Step=1200]\tLoss=0.1048\tacc=0.9639\t2387.5 examples/second\n",
      "[Step=1250]\tLoss=0.0997\tacc=0.9646\t3793.1 examples/second\n",
      "[Step=1300]\tLoss=0.0994\tacc=0.9642\t3878.2 examples/second\n",
      "[Step=1350]\tLoss=0.1000\tacc=0.9638\t3979.7 examples/second\n",
      "Test Loss=0.3611, Test acc=0.9027\n",
      "Saving...\n",
      "\n",
      "Epoch: 7\n",
      "[Step=1400]\tLoss=0.0966\tacc=0.9658\t2450.4 examples/second\n",
      "[Step=1450]\tLoss=0.0976\tacc=0.9644\t3942.6 examples/second\n",
      "[Step=1500]\tLoss=0.0968\tacc=0.9648\t3943.1 examples/second\n",
      "[Step=1550]\tLoss=0.0970\tacc=0.9648\t3895.2 examples/second\n",
      "Test Loss=0.3647, Test acc=0.9030\n",
      "Saving...\n",
      "\n",
      "Epoch: 8\n",
      "[Step=1600]\tLoss=0.0978\tacc=0.9661\t2483.5 examples/second\n",
      "[Step=1650]\tLoss=0.0969\tacc=0.9655\t3910.9 examples/second\n",
      "[Step=1700]\tLoss=0.0965\tacc=0.9660\t3940.7 examples/second\n",
      "[Step=1750]\tLoss=0.0959\tacc=0.9659\t3968.2 examples/second\n",
      "Test Loss=0.3678, Test acc=0.9050\n",
      "Saving...\n",
      "\n",
      "Epoch: 9\n",
      "[Step=1800]\tLoss=0.0982\tacc=0.9633\t2441.7 examples/second\n",
      "[Step=1850]\tLoss=0.0949\tacc=0.9659\t3964.9 examples/second\n",
      "[Step=1900]\tLoss=0.0963\tacc=0.9653\t3951.8 examples/second\n",
      "[Step=1950]\tLoss=0.0941\tacc=0.9660\t3964.0 examples/second\n",
      "Test Loss=0.3610, Test acc=0.9058\n",
      "Saving...\n",
      "\n",
      "Epoch: 10\n",
      "[Step=2000]\tLoss=0.0900\tacc=0.9682\t2486.1 examples/second\n",
      "[Step=2050]\tLoss=0.0921\tacc=0.9673\t3991.1 examples/second\n",
      "[Step=2100]\tLoss=0.0932\tacc=0.9671\t3911.7 examples/second\n",
      "[Step=2150]\tLoss=0.0939\tacc=0.9669\t3984.3 examples/second\n",
      "Test Loss=0.3533, Test acc=0.9054\n",
      "\n",
      "Epoch: 11\n",
      "[Step=2200]\tLoss=0.0851\tacc=0.9701\t2465.7 examples/second\n",
      "[Step=2250]\tLoss=0.0886\tacc=0.9688\t3964.0 examples/second\n",
      "[Step=2300]\tLoss=0.0903\tacc=0.9678\t3949.5 examples/second\n",
      "[Step=2350]\tLoss=0.0903\tacc=0.9678\t3928.5 examples/second\n",
      "Test Loss=0.3609, Test acc=0.9057\n",
      "\n",
      "Epoch: 12\n",
      "[Step=2400]\tLoss=0.0984\tacc=0.9642\t2429.6 examples/second\n",
      "[Step=2450]\tLoss=0.0954\tacc=0.9653\t3980.0 examples/second\n",
      "[Step=2500]\tLoss=0.0928\tacc=0.9663\t3976.6 examples/second\n",
      "Test Loss=0.3636, Test acc=0.9059\n",
      "Saving...\n",
      "\n",
      "Epoch: 13\n",
      "[Step=2550]\tLoss=0.0918\tacc=0.9629\t2370.0 examples/second\n",
      "[Step=2600]\tLoss=0.0917\tacc=0.9672\t3850.4 examples/second\n",
      "[Step=2650]\tLoss=0.0940\tacc=0.9661\t3843.1 examples/second\n",
      "[Step=2700]\tLoss=0.0907\tacc=0.9679\t3720.3 examples/second\n",
      "Test Loss=0.3537, Test acc=0.9063\n",
      "Saving...\n",
      "\n",
      "Epoch: 14\n",
      "[Step=2750]\tLoss=0.0863\tacc=0.9694\t2422.1 examples/second\n",
      "[Step=2800]\tLoss=0.0854\tacc=0.9696\t3835.8 examples/second\n",
      "[Step=2850]\tLoss=0.0867\tacc=0.9687\t3805.5 examples/second\n",
      "[Step=2900]\tLoss=0.0871\tacc=0.9684\t3975.1 examples/second\n",
      "Test Loss=0.3554, Test acc=0.9052\n",
      "\n",
      "Epoch: 15\n",
      "[Step=2950]\tLoss=0.0848\tacc=0.9719\t2415.3 examples/second\n",
      "[Step=3000]\tLoss=0.0901\tacc=0.9669\t4003.1 examples/second\n",
      "[Step=3050]\tLoss=0.0887\tacc=0.9679\t3983.3 examples/second\n",
      "[Step=3100]\tLoss=0.0876\tacc=0.9686\t3871.5 examples/second\n",
      "Test Loss=0.3582, Test acc=0.9037\n",
      "\n",
      "Epoch: 16\n",
      "[Step=3150]\tLoss=0.0996\tacc=0.9648\t2477.3 examples/second\n",
      "[Step=3200]\tLoss=0.0908\tacc=0.9677\t3943.7 examples/second\n",
      "[Step=3250]\tLoss=0.0909\tacc=0.9675\t3962.1 examples/second\n",
      "[Step=3300]\tLoss=0.0895\tacc=0.9679\t3946.0 examples/second\n",
      "Test Loss=0.3683, Test acc=0.9026\n",
      "\n",
      "Epoch: 17\n",
      "[Step=3350]\tLoss=0.0819\tacc=0.9694\t2471.2 examples/second\n",
      "[Step=3400]\tLoss=0.0858\tacc=0.9697\t3962.9 examples/second\n",
      "[Step=3450]\tLoss=0.0849\tacc=0.9692\t3976.6 examples/second\n",
      "[Step=3500]\tLoss=0.0847\tacc=0.9697\t3810.7 examples/second\n",
      "Test Loss=0.3653, Test acc=0.9018\n",
      "\n",
      "Epoch: 18\n",
      "[Step=3550]\tLoss=0.0825\tacc=0.9698\t2443.0 examples/second\n",
      "[Step=3600]\tLoss=0.0861\tacc=0.9691\t3918.8 examples/second\n",
      "[Step=3650]\tLoss=0.0882\tacc=0.9689\t3865.0 examples/second\n",
      "[Step=3700]\tLoss=0.0868\tacc=0.9695\t3841.1 examples/second\n",
      "Test Loss=0.3578, Test acc=0.9020\n",
      "\n",
      "Epoch: 19\n",
      "[Step=3750]\tLoss=0.0915\tacc=0.9688\t2452.5 examples/second\n",
      "[Step=3800]\tLoss=0.0896\tacc=0.9695\t3975.2 examples/second\n",
      "[Step=3850]\tLoss=0.0881\tacc=0.9691\t3965.9 examples/second\n",
      "[Step=3900]\tLoss=0.0875\tacc=0.9693\t3865.6 examples/second\n",
      "Test Loss=0.3633, Test acc=0.9010\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.3537, Test accuracy=0.9063\n",
      "========================\n",
      "Current Nbits: 4\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.3861, Test accuracy=0.8972\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Epoch: 0\n",
      "[Step=50]\tLoss=0.0683\tacc=0.9762\t3762.7 examples/second\n",
      "[Step=100]\tLoss=0.0681\tacc=0.9772\t3922.8 examples/second\n",
      "[Step=150]\tLoss=0.0680\tacc=0.9768\t3939.7 examples/second\n",
      "Test Loss=0.3347, Test acc=0.9103\n",
      "Saving...\n",
      "\n",
      "Epoch: 1\n",
      "[Step=200]\tLoss=0.0671\tacc=0.9785\t2425.1 examples/second\n",
      "[Step=250]\tLoss=0.0629\tacc=0.9782\t3854.2 examples/second\n",
      "[Step=300]\tLoss=0.0635\tacc=0.9784\t3894.0 examples/second\n",
      "[Step=350]\tLoss=0.0644\tacc=0.9779\t3842.1 examples/second\n",
      "Test Loss=0.3346, Test acc=0.9101\n",
      "\n",
      "Epoch: 2\n",
      "[Step=400]\tLoss=0.0648\tacc=0.9766\t2388.6 examples/second\n",
      "[Step=450]\tLoss=0.0614\tacc=0.9798\t3957.5 examples/second\n",
      "[Step=500]\tLoss=0.0616\tacc=0.9799\t3976.0 examples/second\n",
      "[Step=550]\tLoss=0.0616\tacc=0.9797\t3779.9 examples/second\n",
      "Test Loss=0.3339, Test acc=0.9108\n",
      "Saving...\n",
      "\n",
      "Epoch: 3\n",
      "[Step=600]\tLoss=0.0675\tacc=0.9788\t2422.8 examples/second\n",
      "[Step=650]\tLoss=0.0576\tacc=0.9803\t3984.8 examples/second\n",
      "[Step=700]\tLoss=0.0587\tacc=0.9802\t3967.4 examples/second\n",
      "[Step=750]\tLoss=0.0581\tacc=0.9803\t3961.1 examples/second\n",
      "Test Loss=0.3314, Test acc=0.9114\n",
      "Saving...\n",
      "\n",
      "Epoch: 4\n",
      "[Step=800]\tLoss=0.0611\tacc=0.9775\t2442.6 examples/second\n",
      "[Step=850]\tLoss=0.0551\tacc=0.9806\t3905.1 examples/second\n",
      "[Step=900]\tLoss=0.0555\tacc=0.9807\t3907.6 examples/second\n",
      "[Step=950]\tLoss=0.0565\tacc=0.9804\t3917.1 examples/second\n",
      "Test Loss=0.3331, Test acc=0.9110\n",
      "\n",
      "Epoch: 5\n",
      "[Step=1000]\tLoss=0.0613\tacc=0.9783\t2384.4 examples/second\n",
      "[Step=1050]\tLoss=0.0599\tacc=0.9792\t3951.5 examples/second\n",
      "[Step=1100]\tLoss=0.0601\tacc=0.9793\t3828.8 examples/second\n",
      "[Step=1150]\tLoss=0.0598\tacc=0.9793\t3795.4 examples/second\n",
      "Test Loss=0.3305, Test acc=0.9114\n",
      "\n",
      "Epoch: 6\n",
      "[Step=1200]\tLoss=0.0608\tacc=0.9798\t2460.0 examples/second\n",
      "[Step=1250]\tLoss=0.0586\tacc=0.9812\t3956.3 examples/second\n",
      "[Step=1300]\tLoss=0.0603\tacc=0.9799\t3809.6 examples/second\n",
      "[Step=1350]\tLoss=0.0591\tacc=0.9805\t3875.7 examples/second\n",
      "Test Loss=0.3318, Test acc=0.9124\n",
      "Saving...\n",
      "\n",
      "Epoch: 7\n",
      "[Step=1400]\tLoss=0.0515\tacc=0.9849\t2336.2 examples/second\n",
      "[Step=1450]\tLoss=0.0563\tacc=0.9811\t3787.6 examples/second\n",
      "[Step=1500]\tLoss=0.0557\tacc=0.9813\t3713.5 examples/second\n",
      "[Step=1550]\tLoss=0.0575\tacc=0.9804\t3841.9 examples/second\n",
      "Test Loss=0.3322, Test acc=0.9125\n",
      "Saving...\n",
      "\n",
      "Epoch: 8\n",
      "[Step=1600]\tLoss=0.0532\tacc=0.9838\t2411.8 examples/second\n",
      "[Step=1650]\tLoss=0.0539\tacc=0.9831\t3947.5 examples/second\n",
      "[Step=1700]\tLoss=0.0555\tacc=0.9823\t3970.1 examples/second\n",
      "[Step=1750]\tLoss=0.0562\tacc=0.9822\t3949.7 examples/second\n",
      "Test Loss=0.3267, Test acc=0.9133\n",
      "Saving...\n",
      "\n",
      "Epoch: 9\n",
      "[Step=1800]\tLoss=0.0597\tacc=0.9793\t2380.1 examples/second\n",
      "[Step=1850]\tLoss=0.0569\tacc=0.9808\t3788.9 examples/second\n",
      "[Step=1900]\tLoss=0.0553\tacc=0.9811\t3716.9 examples/second\n",
      "[Step=1950]\tLoss=0.0555\tacc=0.9814\t3897.0 examples/second\n",
      "Test Loss=0.3310, Test acc=0.9123\n",
      "\n",
      "Epoch: 10\n",
      "[Step=2000]\tLoss=0.0551\tacc=0.9831\t2399.8 examples/second\n",
      "[Step=2050]\tLoss=0.0554\tacc=0.9824\t3961.0 examples/second\n",
      "[Step=2100]\tLoss=0.0559\tacc=0.9814\t3950.1 examples/second\n",
      "[Step=2150]\tLoss=0.0562\tacc=0.9811\t3738.1 examples/second\n",
      "Test Loss=0.3322, Test acc=0.9113\n",
      "\n",
      "Epoch: 11\n",
      "[Step=2200]\tLoss=0.0574\tacc=0.9811\t2477.5 examples/second\n",
      "[Step=2250]\tLoss=0.0543\tacc=0.9811\t3949.2 examples/second\n",
      "[Step=2300]\tLoss=0.0539\tacc=0.9812\t3921.7 examples/second\n",
      "[Step=2350]\tLoss=0.0547\tacc=0.9815\t3932.6 examples/second\n",
      "Test Loss=0.3334, Test acc=0.9120\n",
      "\n",
      "Epoch: 12\n",
      "[Step=2400]\tLoss=0.0539\tacc=0.9831\t2415.5 examples/second\n",
      "[Step=2450]\tLoss=0.0530\tacc=0.9832\t3884.4 examples/second\n",
      "[Step=2500]\tLoss=0.0545\tacc=0.9822\t3864.9 examples/second\n",
      "Test Loss=0.3313, Test acc=0.9116\n",
      "\n",
      "Epoch: 13\n",
      "[Step=2550]\tLoss=0.0621\tacc=0.9805\t2443.0 examples/second\n",
      "[Step=2600]\tLoss=0.0553\tacc=0.9811\t3956.6 examples/second\n",
      "[Step=2650]\tLoss=0.0536\tacc=0.9821\t3857.9 examples/second\n",
      "[Step=2700]\tLoss=0.0540\tacc=0.9820\t3760.3 examples/second\n",
      "Test Loss=0.3267, Test acc=0.9118\n",
      "\n",
      "Epoch: 14\n",
      "[Step=2750]\tLoss=0.0578\tacc=0.9818\t2447.3 examples/second\n",
      "[Step=2800]\tLoss=0.0523\tacc=0.9829\t3860.8 examples/second\n",
      "[Step=2850]\tLoss=0.0540\tacc=0.9826\t3838.5 examples/second\n",
      "[Step=2900]\tLoss=0.0525\tacc=0.9829\t3864.9 examples/second\n",
      "Test Loss=0.3326, Test acc=0.9125\n",
      "\n",
      "Epoch: 15\n",
      "[Step=2950]\tLoss=0.0573\tacc=0.9789\t2494.8 examples/second\n",
      "[Step=3000]\tLoss=0.0548\tacc=0.9821\t3875.6 examples/second\n",
      "[Step=3050]\tLoss=0.0559\tacc=0.9819\t3889.4 examples/second\n",
      "[Step=3100]\tLoss=0.0559\tacc=0.9817\t3956.0 examples/second\n",
      "Test Loss=0.3303, Test acc=0.9136\n",
      "Saving...\n",
      "\n",
      "Epoch: 16\n",
      "[Step=3150]\tLoss=0.0494\tacc=0.9810\t2478.0 examples/second\n",
      "[Step=3200]\tLoss=0.0541\tacc=0.9827\t3962.9 examples/second\n",
      "[Step=3250]\tLoss=0.0556\tacc=0.9819\t3723.8 examples/second\n",
      "[Step=3300]\tLoss=0.0543\tacc=0.9825\t3899.7 examples/second\n",
      "Test Loss=0.3327, Test acc=0.9136\n",
      "\n",
      "Epoch: 17\n",
      "[Step=3350]\tLoss=0.0545\tacc=0.9822\t2464.0 examples/second\n",
      "[Step=3400]\tLoss=0.0517\tacc=0.9831\t3811.9 examples/second\n",
      "[Step=3450]\tLoss=0.0518\tacc=0.9828\t3743.8 examples/second\n",
      "[Step=3500]\tLoss=0.0532\tacc=0.9820\t3825.9 examples/second\n",
      "Test Loss=0.3286, Test acc=0.9132\n",
      "\n",
      "Epoch: 18\n",
      "[Step=3550]\tLoss=0.0531\tacc=0.9819\t2456.6 examples/second\n",
      "[Step=3600]\tLoss=0.0531\tacc=0.9821\t4005.9 examples/second\n",
      "[Step=3650]\tLoss=0.0533\tacc=0.9822\t3764.9 examples/second\n",
      "[Step=3700]\tLoss=0.0525\tacc=0.9827\t3981.9 examples/second\n",
      "Test Loss=0.3353, Test acc=0.9139\n",
      "Saving...\n",
      "\n",
      "Epoch: 19\n",
      "[Step=3750]\tLoss=0.0525\tacc=0.9836\t2496.1 examples/second\n",
      "[Step=3800]\tLoss=0.0529\tacc=0.9826\t3965.7 examples/second\n",
      "[Step=3850]\tLoss=0.0514\tacc=0.9828\t3957.3 examples/second\n",
      "[Step=3900]\tLoss=0.0521\tacc=0.9830\t3972.5 examples/second\n",
      "Test Loss=0.3371, Test acc=0.9148\n",
      "Saving...\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.3371, Test accuracy=0.9148\n",
      "========================\n",
      "Current Nbits: 5\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.3390, Test accuracy=0.9112\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Epoch: 0\n",
      "[Step=50]\tLoss=0.0524\tacc=0.9830\t3601.1 examples/second\n",
      "[Step=100]\tLoss=0.0501\tacc=0.9840\t3862.2 examples/second\n",
      "[Step=150]\tLoss=0.0507\tacc=0.9838\t3931.5 examples/second\n",
      "Test Loss=0.3276, Test acc=0.9141\n",
      "Saving...\n",
      "\n",
      "Epoch: 1\n",
      "[Step=200]\tLoss=0.0540\tacc=0.9834\t2410.2 examples/second\n",
      "[Step=250]\tLoss=0.0510\tacc=0.9835\t3963.9 examples/second\n",
      "[Step=300]\tLoss=0.0508\tacc=0.9837\t3841.6 examples/second\n",
      "[Step=350]\tLoss=0.0515\tacc=0.9831\t3953.8 examples/second\n",
      "Test Loss=0.3266, Test acc=0.9140\n",
      "\n",
      "Epoch: 2\n",
      "[Step=400]\tLoss=0.0496\tacc=0.9829\t2451.5 examples/second\n",
      "[Step=450]\tLoss=0.0508\tacc=0.9837\t3935.1 examples/second\n",
      "[Step=500]\tLoss=0.0509\tacc=0.9835\t3928.7 examples/second\n",
      "[Step=550]\tLoss=0.0505\tacc=0.9840\t3967.0 examples/second\n",
      "Test Loss=0.3272, Test acc=0.9143\n",
      "Saving...\n",
      "\n",
      "Epoch: 3\n",
      "[Step=600]\tLoss=0.0540\tacc=0.9840\t2443.2 examples/second\n",
      "[Step=650]\tLoss=0.0503\tacc=0.9843\t3868.0 examples/second\n",
      "[Step=700]\tLoss=0.0489\tacc=0.9853\t3721.9 examples/second\n",
      "[Step=750]\tLoss=0.0495\tacc=0.9851\t3947.9 examples/second\n",
      "Test Loss=0.3269, Test acc=0.9140\n",
      "\n",
      "Epoch: 4\n",
      "[Step=800]\tLoss=0.0569\tacc=0.9814\t2444.8 examples/second\n",
      "[Step=850]\tLoss=0.0518\tacc=0.9828\t3774.8 examples/second\n",
      "[Step=900]\tLoss=0.0515\tacc=0.9832\t3894.8 examples/second\n",
      "[Step=950]\tLoss=0.0508\tacc=0.9834\t3849.8 examples/second\n",
      "Test Loss=0.3242, Test acc=0.9147\n",
      "Saving...\n",
      "\n",
      "Epoch: 5\n",
      "[Step=1000]\tLoss=0.0521\tacc=0.9814\t2358.9 examples/second\n",
      "[Step=1050]\tLoss=0.0505\tacc=0.9833\t3708.3 examples/second\n",
      "[Step=1100]\tLoss=0.0493\tacc=0.9838\t3631.7 examples/second\n",
      "[Step=1150]\tLoss=0.0497\tacc=0.9839\t3652.4 examples/second\n",
      "Test Loss=0.3262, Test acc=0.9151\n",
      "Saving...\n",
      "\n",
      "Epoch: 6\n",
      "[Step=1200]\tLoss=0.0517\tacc=0.9834\t2470.2 examples/second\n",
      "[Step=1250]\tLoss=0.0500\tacc=0.9842\t3971.9 examples/second\n",
      "[Step=1300]\tLoss=0.0492\tacc=0.9848\t3936.8 examples/second\n",
      "[Step=1350]\tLoss=0.0497\tacc=0.9845\t3930.1 examples/second\n",
      "Test Loss=0.3266, Test acc=0.9144\n",
      "\n",
      "Epoch: 7\n",
      "[Step=1400]\tLoss=0.0496\tacc=0.9840\t2430.3 examples/second\n",
      "[Step=1450]\tLoss=0.0489\tacc=0.9842\t3849.6 examples/second\n",
      "[Step=1500]\tLoss=0.0490\tacc=0.9842\t3813.7 examples/second\n",
      "[Step=1550]\tLoss=0.0490\tacc=0.9843\t3818.5 examples/second\n",
      "Test Loss=0.3281, Test acc=0.9157\n",
      "Saving...\n",
      "\n",
      "Epoch: 8\n",
      "[Step=1600]\tLoss=0.0537\tacc=0.9819\t2360.3 examples/second\n",
      "[Step=1650]\tLoss=0.0499\tacc=0.9839\t3752.4 examples/second\n",
      "[Step=1700]\tLoss=0.0507\tacc=0.9832\t3905.0 examples/second\n",
      "[Step=1750]\tLoss=0.0505\tacc=0.9837\t3890.4 examples/second\n",
      "Test Loss=0.3272, Test acc=0.9159\n",
      "Saving...\n",
      "\n",
      "Epoch: 9\n",
      "[Step=1800]\tLoss=0.0447\tacc=0.9859\t2353.3 examples/second\n",
      "[Step=1850]\tLoss=0.0464\tacc=0.9852\t3925.9 examples/second\n",
      "[Step=1900]\tLoss=0.0477\tacc=0.9847\t3943.4 examples/second\n",
      "[Step=1950]\tLoss=0.0478\tacc=0.9848\t3892.3 examples/second\n",
      "Test Loss=0.3302, Test acc=0.9157\n",
      "\n",
      "Epoch: 10\n",
      "[Step=2000]\tLoss=0.0498\tacc=0.9834\t2434.0 examples/second\n",
      "[Step=2050]\tLoss=0.0502\tacc=0.9843\t3726.6 examples/second\n",
      "[Step=2100]\tLoss=0.0511\tacc=0.9835\t3916.4 examples/second\n",
      "[Step=2150]\tLoss=0.0507\tacc=0.9835\t3925.8 examples/second\n",
      "Test Loss=0.3284, Test acc=0.9148\n",
      "\n",
      "Epoch: 11\n",
      "[Step=2200]\tLoss=0.0463\tacc=0.9851\t2475.2 examples/second\n",
      "[Step=2250]\tLoss=0.0466\tacc=0.9855\t3847.6 examples/second\n",
      "[Step=2300]\tLoss=0.0473\tacc=0.9850\t3719.7 examples/second\n",
      "[Step=2350]\tLoss=0.0477\tacc=0.9849\t3888.2 examples/second\n",
      "Test Loss=0.3304, Test acc=0.9141\n",
      "\n",
      "Epoch: 12\n",
      "[Step=2400]\tLoss=0.0475\tacc=0.9847\t2392.7 examples/second\n",
      "[Step=2450]\tLoss=0.0504\tacc=0.9828\t3943.1 examples/second\n",
      "[Step=2500]\tLoss=0.0497\tacc=0.9835\t3952.6 examples/second\n",
      "Test Loss=0.3294, Test acc=0.9143\n",
      "\n",
      "Epoch: 13\n",
      "[Step=2550]\tLoss=0.0426\tacc=0.9902\t2295.3 examples/second\n",
      "[Step=2600]\tLoss=0.0502\tacc=0.9843\t3861.1 examples/second\n",
      "[Step=2650]\tLoss=0.0476\tacc=0.9846\t3762.4 examples/second\n",
      "[Step=2700]\tLoss=0.0468\tacc=0.9849\t3916.5 examples/second\n",
      "Test Loss=0.3292, Test acc=0.9136\n",
      "\n",
      "Epoch: 14\n",
      "[Step=2750]\tLoss=0.0408\tacc=0.9870\t2442.3 examples/second\n",
      "[Step=2800]\tLoss=0.0447\tacc=0.9865\t3896.8 examples/second\n",
      "[Step=2850]\tLoss=0.0461\tacc=0.9862\t3961.3 examples/second\n",
      "[Step=2900]\tLoss=0.0459\tacc=0.9861\t3970.9 examples/second\n",
      "Test Loss=0.3303, Test acc=0.9129\n",
      "\n",
      "Epoch: 15\n",
      "[Step=2950]\tLoss=0.0400\tacc=0.9887\t2434.3 examples/second\n",
      "[Step=3000]\tLoss=0.0460\tacc=0.9852\t3943.8 examples/second\n",
      "[Step=3050]\tLoss=0.0447\tacc=0.9860\t3978.0 examples/second\n",
      "[Step=3100]\tLoss=0.0458\tacc=0.9856\t3926.5 examples/second\n",
      "Test Loss=0.3328, Test acc=0.9129\n",
      "\n",
      "Epoch: 16\n",
      "[Step=3150]\tLoss=0.0476\tacc=0.9860\t2447.1 examples/second\n",
      "[Step=3200]\tLoss=0.0456\tacc=0.9862\t3782.9 examples/second\n",
      "[Step=3250]\tLoss=0.0474\tacc=0.9854\t3942.5 examples/second\n",
      "[Step=3300]\tLoss=0.0470\tacc=0.9852\t3868.1 examples/second\n",
      "Test Loss=0.3299, Test acc=0.9141\n",
      "\n",
      "Epoch: 17\n",
      "[Step=3350]\tLoss=0.0466\tacc=0.9844\t2397.6 examples/second\n",
      "[Step=3400]\tLoss=0.0473\tacc=0.9838\t3878.5 examples/second\n",
      "[Step=3450]\tLoss=0.0463\tacc=0.9848\t3714.3 examples/second\n",
      "[Step=3500]\tLoss=0.0472\tacc=0.9846\t3712.5 examples/second\n",
      "Test Loss=0.3343, Test acc=0.9158\n",
      "\n",
      "Epoch: 18\n",
      "[Step=3550]\tLoss=0.0458\tacc=0.9846\t2440.2 examples/second\n",
      "[Step=3600]\tLoss=0.0455\tacc=0.9851\t3839.5 examples/second\n",
      "[Step=3650]\tLoss=0.0457\tacc=0.9854\t3959.8 examples/second\n",
      "[Step=3700]\tLoss=0.0464\tacc=0.9852\t3938.9 examples/second\n",
      "Test Loss=0.3305, Test acc=0.9147\n",
      "\n",
      "Epoch: 19\n",
      "[Step=3750]\tLoss=0.0464\tacc=0.9850\t2385.2 examples/second\n",
      "[Step=3800]\tLoss=0.0456\tacc=0.9855\t3967.0 examples/second\n",
      "[Step=3850]\tLoss=0.0461\tacc=0.9856\t3739.1 examples/second\n",
      "[Step=3900]\tLoss=0.0473\tacc=0.9850\t3745.4 examples/second\n",
      "Test Loss=0.3332, Test acc=0.9141\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.3272, Test accuracy=0.9159\n",
      "========================\n",
      "Current Nbits: 6\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.3364, Test accuracy=0.9145\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Epoch: 0\n",
      "[Step=50]\tLoss=0.0517\tacc=0.9832\t3736.0 examples/second\n",
      "[Step=100]\tLoss=0.0523\tacc=0.9832\t3758.3 examples/second\n",
      "[Step=150]\tLoss=0.0496\tacc=0.9846\t3643.8 examples/second\n",
      "Test Loss=0.3245, Test acc=0.9160\n",
      "Saving...\n",
      "\n",
      "Epoch: 1\n",
      "[Step=200]\tLoss=0.0498\tacc=0.9824\t2388.1 examples/second\n",
      "[Step=250]\tLoss=0.0480\tacc=0.9851\t3939.0 examples/second\n",
      "[Step=300]\tLoss=0.0476\tacc=0.9855\t3937.4 examples/second\n",
      "[Step=350]\tLoss=0.0476\tacc=0.9858\t3915.5 examples/second\n",
      "Test Loss=0.3268, Test acc=0.9157\n",
      "\n",
      "Epoch: 2\n",
      "[Step=400]\tLoss=0.0463\tacc=0.9878\t2437.8 examples/second\n",
      "[Step=450]\tLoss=0.0503\tacc=0.9840\t3710.5 examples/second\n",
      "[Step=500]\tLoss=0.0493\tacc=0.9843\t3926.7 examples/second\n",
      "[Step=550]\tLoss=0.0487\tacc=0.9840\t3883.1 examples/second\n",
      "Test Loss=0.3293, Test acc=0.9153\n",
      "\n",
      "Epoch: 3\n",
      "[Step=600]\tLoss=0.0478\tacc=0.9840\t2454.4 examples/second\n",
      "[Step=650]\tLoss=0.0521\tacc=0.9839\t3832.3 examples/second\n",
      "[Step=700]\tLoss=0.0491\tacc=0.9843\t3974.4 examples/second\n",
      "[Step=750]\tLoss=0.0482\tacc=0.9847\t3937.2 examples/second\n",
      "Test Loss=0.3272, Test acc=0.9145\n",
      "\n",
      "Epoch: 4\n",
      "[Step=800]\tLoss=0.0390\tacc=0.9880\t2484.5 examples/second\n",
      "[Step=850]\tLoss=0.0467\tacc=0.9842\t3905.5 examples/second\n",
      "[Step=900]\tLoss=0.0465\tacc=0.9847\t3947.7 examples/second\n",
      "[Step=950]\tLoss=0.0469\tacc=0.9848\t3932.8 examples/second\n",
      "Test Loss=0.3287, Test acc=0.9135\n",
      "\n",
      "Epoch: 5\n",
      "[Step=1000]\tLoss=0.0488\tacc=0.9836\t2441.4 examples/second\n",
      "[Step=1050]\tLoss=0.0492\tacc=0.9845\t3953.1 examples/second\n",
      "[Step=1100]\tLoss=0.0489\tacc=0.9842\t3979.9 examples/second\n",
      "[Step=1150]\tLoss=0.0477\tacc=0.9849\t3976.4 examples/second\n",
      "Test Loss=0.3279, Test acc=0.9143\n",
      "\n",
      "Epoch: 6\n",
      "[Step=1200]\tLoss=0.0495\tacc=0.9852\t2439.7 examples/second\n",
      "[Step=1250]\tLoss=0.0483\tacc=0.9848\t3894.4 examples/second\n",
      "[Step=1300]\tLoss=0.0482\tacc=0.9847\t3683.9 examples/second\n",
      "[Step=1350]\tLoss=0.0484\tacc=0.9847\t3794.1 examples/second\n",
      "Test Loss=0.3272, Test acc=0.9148\n",
      "\n",
      "Epoch: 7\n",
      "[Step=1400]\tLoss=0.0498\tacc=0.9847\t2380.2 examples/second\n",
      "[Step=1450]\tLoss=0.0483\tacc=0.9855\t3852.3 examples/second\n",
      "[Step=1500]\tLoss=0.0481\tacc=0.9856\t3831.8 examples/second\n",
      "[Step=1550]\tLoss=0.0481\tacc=0.9855\t3780.2 examples/second\n",
      "Test Loss=0.3277, Test acc=0.9136\n",
      "\n",
      "Epoch: 8\n",
      "[Step=1600]\tLoss=0.0434\tacc=0.9865\t2377.8 examples/second\n",
      "[Step=1650]\tLoss=0.0463\tacc=0.9849\t3961.6 examples/second\n",
      "[Step=1700]\tLoss=0.0473\tacc=0.9848\t3955.6 examples/second\n",
      "[Step=1750]\tLoss=0.0475\tacc=0.9844\t3915.2 examples/second\n",
      "Test Loss=0.3278, Test acc=0.9155\n",
      "\n",
      "Epoch: 9\n",
      "[Step=1800]\tLoss=0.0475\tacc=0.9858\t2447.3 examples/second\n",
      "[Step=1850]\tLoss=0.0471\tacc=0.9858\t3896.1 examples/second\n",
      "[Step=1900]\tLoss=0.0463\tacc=0.9857\t3899.3 examples/second\n",
      "[Step=1950]\tLoss=0.0470\tacc=0.9851\t3844.6 examples/second\n",
      "Test Loss=0.3290, Test acc=0.9152\n",
      "\n",
      "Epoch: 10\n",
      "[Step=2000]\tLoss=0.0468\tacc=0.9851\t2417.6 examples/second\n",
      "[Step=2050]\tLoss=0.0449\tacc=0.9855\t3959.6 examples/second\n",
      "[Step=2100]\tLoss=0.0465\tacc=0.9852\t3949.7 examples/second\n",
      "[Step=2150]\tLoss=0.0471\tacc=0.9849\t3931.9 examples/second\n",
      "Test Loss=0.3309, Test acc=0.9147\n",
      "\n",
      "Epoch: 11\n",
      "[Step=2200]\tLoss=0.0463\tacc=0.9847\t2321.6 examples/second\n",
      "[Step=2250]\tLoss=0.0447\tacc=0.9860\t3887.7 examples/second\n",
      "[Step=2300]\tLoss=0.0459\tacc=0.9850\t3918.5 examples/second\n",
      "[Step=2350]\tLoss=0.0461\tacc=0.9849\t3939.1 examples/second\n",
      "Test Loss=0.3293, Test acc=0.9142\n",
      "\n",
      "Epoch: 12\n",
      "[Step=2400]\tLoss=0.0437\tacc=0.9856\t2487.4 examples/second\n",
      "[Step=2450]\tLoss=0.0445\tacc=0.9856\t3885.3 examples/second\n",
      "[Step=2500]\tLoss=0.0461\tacc=0.9849\t3844.5 examples/second\n",
      "Test Loss=0.3296, Test acc=0.9137\n",
      "\n",
      "Epoch: 13\n",
      "[Step=2550]\tLoss=0.0483\tacc=0.9824\t2469.5 examples/second\n",
      "[Step=2600]\tLoss=0.0428\tacc=0.9881\t3946.5 examples/second\n",
      "[Step=2650]\tLoss=0.0444\tacc=0.9869\t3998.1 examples/second\n",
      "[Step=2700]\tLoss=0.0447\tacc=0.9865\t3948.5 examples/second\n",
      "Test Loss=0.3325, Test acc=0.9127\n",
      "\n",
      "Epoch: 14\n",
      "[Step=2750]\tLoss=0.0437\tacc=0.9863\t2435.4 examples/second\n",
      "[Step=2800]\tLoss=0.0448\tacc=0.9856\t3884.8 examples/second\n",
      "[Step=2850]\tLoss=0.0450\tacc=0.9862\t3972.6 examples/second\n",
      "[Step=2900]\tLoss=0.0443\tacc=0.9861\t3960.8 examples/second\n",
      "Test Loss=0.3326, Test acc=0.9151\n",
      "\n",
      "Epoch: 15\n",
      "[Step=2950]\tLoss=0.0482\tacc=0.9859\t2446.8 examples/second\n",
      "[Step=3000]\tLoss=0.0473\tacc=0.9855\t3961.0 examples/second\n",
      "[Step=3050]\tLoss=0.0472\tacc=0.9854\t3921.5 examples/second\n",
      "[Step=3100]\tLoss=0.0457\tacc=0.9860\t3884.9 examples/second\n",
      "Test Loss=0.3329, Test acc=0.9144\n",
      "\n",
      "Epoch: 16\n",
      "[Step=3150]\tLoss=0.0432\tacc=0.9880\t2492.5 examples/second\n",
      "[Step=3200]\tLoss=0.0440\tacc=0.9861\t3948.7 examples/second\n",
      "[Step=3250]\tLoss=0.0429\tacc=0.9868\t3941.4 examples/second\n",
      "[Step=3300]\tLoss=0.0432\tacc=0.9868\t3896.9 examples/second\n",
      "Test Loss=0.3348, Test acc=0.9134\n",
      "\n",
      "Epoch: 17\n",
      "[Step=3350]\tLoss=0.0442\tacc=0.9857\t2442.0 examples/second\n",
      "[Step=3400]\tLoss=0.0438\tacc=0.9863\t3893.4 examples/second\n",
      "[Step=3450]\tLoss=0.0462\tacc=0.9853\t3782.5 examples/second\n",
      "[Step=3500]\tLoss=0.0451\tacc=0.9857\t3951.6 examples/second\n",
      "Test Loss=0.3328, Test acc=0.9143\n",
      "\n",
      "Epoch: 18\n",
      "[Step=3550]\tLoss=0.0477\tacc=0.9846\t2468.1 examples/second\n",
      "[Step=3600]\tLoss=0.0440\tacc=0.9854\t4000.0 examples/second\n",
      "[Step=3650]\tLoss=0.0442\tacc=0.9851\t3954.5 examples/second\n",
      "[Step=3700]\tLoss=0.0448\tacc=0.9853\t4012.5 examples/second\n",
      "Test Loss=0.3361, Test acc=0.9147\n",
      "\n",
      "Epoch: 19\n",
      "[Step=3750]\tLoss=0.0435\tacc=0.9868\t2443.9 examples/second\n",
      "[Step=3800]\tLoss=0.0418\tacc=0.9870\t3904.8 examples/second\n",
      "[Step=3850]\tLoss=0.0431\tacc=0.9868\t3951.9 examples/second\n",
      "[Step=3900]\tLoss=0.0444\tacc=0.9861\t3961.5 examples/second\n",
      "Test Loss=0.3371, Test acc=0.9128\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.3245, Test accuracy=0.9160\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "for i in [2, 3, 4, 5, 6]:\n",
    "    print(f\"Current Nbits: {i}\")\n",
    "    net = ResNetCIFAR(num_layers=20, Nbits=i)\n",
    "    net = net.to(device)\n",
    "    net.load_state_dict(torch.load(\"pretrained_model.pt\"))\n",
    "    test(net)\n",
    "\n",
    "    # Quantized model finetuning\n",
    "    finetune(net, epochs=20, batch_size=256, lr=0.002, reg=1e-4)   \n",
    "\n",
    "    # Load the model with best accuracy\n",
    "    net.load_state_dict(torch.load(\"quantized_net_after_finetune.pt\"))\n",
    "    test(net)\n",
    "    print('========================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab3 (d) Quantize pruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define quantized model and load weight\n",
    "# Nbits = 3 #Change this value to finish (d)\n",
    "\n",
    "# net = ResNetCIFAR(num_layers=20, Nbits=Nbits)\n",
    "# net = net.to(device)\n",
    "# net.load_state_dict(torch.load(\"net_after_global_iterative_prune.pt\"))\n",
    "# test(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Quantized model finetuning\n",
    "# finetune(net, epochs=20, batch_size=256, lr=0.002, reg=1e-4)\n",
    "\n",
    "# # Load the model with best accuracy\n",
    "# net.load_state_dict(torch.load(\"quantized_net_after_finetune.pt\"))\n",
    "# test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Nbits: 2\n",
      "Files already downloaded and verified\n",
      "Test Loss=6885.1971, Test accuracy=0.1000\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Epoch: 0\n",
      "[Step=50]\tLoss=2.2583\tacc=0.1446\t3741.5 examples/second\n",
      "[Step=100]\tLoss=2.2210\tacc=0.1536\t3906.5 examples/second\n",
      "[Step=150]\tLoss=2.2033\tacc=0.1588\t3930.5 examples/second\n",
      "Test Loss=2.1227, Test acc=0.1802\n",
      "Saving...\n",
      "\n",
      "Epoch: 1\n",
      "[Step=200]\tLoss=2.1479\tacc=0.1768\t2454.7 examples/second\n",
      "[Step=250]\tLoss=2.1307\tacc=0.1908\t3941.8 examples/second\n",
      "[Step=300]\tLoss=2.1206\tacc=0.2015\t3911.8 examples/second\n",
      "[Step=350]\tLoss=2.1140\tacc=0.2064\t3864.2 examples/second\n",
      "Test Loss=2.0870, Test acc=0.2248\n",
      "Saving...\n",
      "\n",
      "Epoch: 2\n",
      "[Step=400]\tLoss=2.0746\tacc=0.2285\t2432.6 examples/second\n",
      "[Step=450]\tLoss=2.0763\tacc=0.2298\t3947.0 examples/second\n",
      "[Step=500]\tLoss=2.0740\tacc=0.2349\t3883.7 examples/second\n",
      "[Step=550]\tLoss=2.0701\tacc=0.2363\t3790.4 examples/second\n",
      "Test Loss=2.0614, Test acc=0.2377\n",
      "Saving...\n",
      "\n",
      "Epoch: 3\n",
      "[Step=600]\tLoss=2.0567\tacc=0.2389\t2372.6 examples/second\n",
      "[Step=650]\tLoss=2.0550\tacc=0.2433\t3763.3 examples/second\n",
      "[Step=700]\tLoss=2.0476\tacc=0.2472\t3971.8 examples/second\n",
      "[Step=750]\tLoss=2.0415\tacc=0.2492\t3956.9 examples/second\n",
      "Test Loss=2.0501, Test acc=0.2459\n",
      "Saving...\n",
      "\n",
      "Epoch: 4\n",
      "[Step=800]\tLoss=2.0206\tacc=0.2603\t2454.3 examples/second\n",
      "[Step=850]\tLoss=2.0255\tacc=0.2545\t3702.1 examples/second\n",
      "[Step=900]\tLoss=2.0185\tacc=0.2575\t3856.9 examples/second\n",
      "[Step=950]\tLoss=2.0170\tacc=0.2596\t3744.1 examples/second\n",
      "Test Loss=2.0415, Test acc=0.2484\n",
      "Saving...\n",
      "\n",
      "Epoch: 5\n",
      "[Step=1000]\tLoss=2.0135\tacc=0.2580\t2305.0 examples/second\n",
      "[Step=1050]\tLoss=1.9957\tacc=0.2699\t3914.9 examples/second\n",
      "[Step=1100]\tLoss=1.9919\tacc=0.2707\t3938.4 examples/second\n",
      "[Step=1150]\tLoss=1.9913\tacc=0.2691\t3946.1 examples/second\n",
      "Test Loss=1.9833, Test acc=0.2730\n",
      "Saving...\n",
      "\n",
      "Epoch: 6\n",
      "[Step=1200]\tLoss=1.9873\tacc=0.2710\t2482.9 examples/second\n",
      "[Step=1250]\tLoss=1.9830\tacc=0.2760\t3885.0 examples/second\n",
      "[Step=1300]\tLoss=1.9816\tacc=0.2745\t3925.5 examples/second\n",
      "[Step=1350]\tLoss=1.9780\tacc=0.2763\t3942.6 examples/second\n",
      "Test Loss=1.9525, Test acc=0.2914\n",
      "Saving...\n",
      "\n",
      "Epoch: 7\n",
      "[Step=1400]\tLoss=1.9557\tacc=0.2815\t2419.8 examples/second\n",
      "[Step=1450]\tLoss=1.9522\tacc=0.2827\t3929.4 examples/second\n",
      "[Step=1500]\tLoss=1.9524\tacc=0.2823\t3939.4 examples/second\n",
      "[Step=1550]\tLoss=1.9510\tacc=0.2833\t3955.6 examples/second\n",
      "Test Loss=1.9504, Test acc=0.2767\n",
      "\n",
      "Epoch: 8\n",
      "[Step=1600]\tLoss=1.9380\tacc=0.2958\t2457.3 examples/second\n",
      "[Step=1650]\tLoss=1.9339\tacc=0.2906\t3636.3 examples/second\n",
      "[Step=1700]\tLoss=1.9308\tacc=0.2901\t3802.2 examples/second\n",
      "[Step=1750]\tLoss=1.9279\tacc=0.2918\t3896.3 examples/second\n",
      "Test Loss=2.0096, Test acc=0.2743\n",
      "\n",
      "Epoch: 9\n",
      "[Step=1800]\tLoss=1.9170\tacc=0.2942\t2482.6 examples/second\n",
      "[Step=1850]\tLoss=1.9139\tacc=0.2927\t3930.9 examples/second\n",
      "[Step=1900]\tLoss=1.9083\tacc=0.2947\t3983.2 examples/second\n",
      "[Step=1950]\tLoss=1.9056\tacc=0.2960\t3785.7 examples/second\n",
      "Test Loss=1.9525, Test acc=0.2808\n",
      "\n",
      "Epoch: 10\n",
      "[Step=2000]\tLoss=1.8927\tacc=0.2980\t2375.2 examples/second\n",
      "[Step=2050]\tLoss=1.8874\tacc=0.3037\t3799.8 examples/second\n",
      "[Step=2100]\tLoss=1.8846\tacc=0.3054\t3989.4 examples/second\n",
      "[Step=2150]\tLoss=1.8785\tacc=0.3065\t3924.6 examples/second\n",
      "Test Loss=1.8779, Test acc=0.3004\n",
      "Saving...\n",
      "\n",
      "Epoch: 11\n",
      "[Step=2200]\tLoss=1.8537\tacc=0.3167\t2456.5 examples/second\n",
      "[Step=2250]\tLoss=1.8546\tacc=0.3150\t3960.0 examples/second\n",
      "[Step=2300]\tLoss=1.8523\tacc=0.3142\t3934.8 examples/second\n",
      "[Step=2350]\tLoss=1.8500\tacc=0.3149\t3961.1 examples/second\n",
      "Test Loss=1.8608, Test acc=0.3076\n",
      "Saving...\n",
      "\n",
      "Epoch: 12\n",
      "[Step=2400]\tLoss=1.8192\tacc=0.3243\t2333.6 examples/second\n",
      "[Step=2450]\tLoss=1.8205\tacc=0.3249\t3946.5 examples/second\n",
      "[Step=2500]\tLoss=1.8233\tacc=0.3242\t3951.4 examples/second\n",
      "Test Loss=1.8880, Test acc=0.2876\n",
      "\n",
      "Epoch: 13\n",
      "[Step=2550]\tLoss=1.7915\tacc=0.3574\t2431.9 examples/second\n",
      "[Step=2600]\tLoss=1.8151\tacc=0.3223\t3833.4 examples/second\n",
      "[Step=2650]\tLoss=1.8043\tacc=0.3281\t3933.8 examples/second\n",
      "[Step=2700]\tLoss=1.7983\tacc=0.3317\t3959.2 examples/second\n",
      "Test Loss=1.8365, Test acc=0.2984\n",
      "\n",
      "Epoch: 14\n",
      "[Step=2750]\tLoss=1.8086\tacc=0.3385\t2429.9 examples/second\n",
      "[Step=2800]\tLoss=1.7794\tacc=0.3401\t3972.0 examples/second\n",
      "[Step=2850]\tLoss=1.7777\tacc=0.3405\t3949.4 examples/second\n",
      "[Step=2900]\tLoss=1.7709\tacc=0.3442\t3987.7 examples/second\n",
      "Test Loss=1.7799, Test acc=0.3323\n",
      "Saving...\n",
      "\n",
      "Epoch: 15\n",
      "[Step=2950]\tLoss=1.7584\tacc=0.3465\t2393.8 examples/second\n",
      "[Step=3000]\tLoss=1.7562\tacc=0.3468\t3841.8 examples/second\n",
      "[Step=3050]\tLoss=1.7460\tacc=0.3491\t3919.5 examples/second\n",
      "[Step=3100]\tLoss=1.7406\tacc=0.3532\t3970.6 examples/second\n",
      "Test Loss=1.7731, Test acc=0.3409\n",
      "Saving...\n",
      "\n",
      "Epoch: 16\n",
      "[Step=3150]\tLoss=1.7218\tacc=0.3580\t2372.7 examples/second\n",
      "[Step=3200]\tLoss=1.7204\tacc=0.3553\t3863.4 examples/second\n",
      "[Step=3250]\tLoss=1.7179\tacc=0.3575\t3912.9 examples/second\n",
      "[Step=3300]\tLoss=1.7149\tacc=0.3581\t3966.1 examples/second\n",
      "Test Loss=1.7202, Test acc=0.3557\n",
      "Saving...\n",
      "\n",
      "Epoch: 17\n",
      "[Step=3350]\tLoss=1.7098\tacc=0.3668\t2440.2 examples/second\n",
      "[Step=3400]\tLoss=1.6895\tacc=0.3722\t3952.3 examples/second\n",
      "[Step=3450]\tLoss=1.6865\tacc=0.3709\t3909.2 examples/second\n",
      "[Step=3500]\tLoss=1.6891\tacc=0.3691\t3938.9 examples/second\n",
      "Test Loss=1.7836, Test acc=0.3317\n",
      "\n",
      "Epoch: 18\n",
      "[Step=3550]\tLoss=1.6762\tacc=0.3736\t2445.6 examples/second\n",
      "[Step=3600]\tLoss=1.6654\tacc=0.3828\t3978.8 examples/second\n",
      "[Step=3650]\tLoss=1.6702\tacc=0.3787\t3961.0 examples/second\n",
      "[Step=3700]\tLoss=1.6666\tacc=0.3799\t3984.3 examples/second\n",
      "Test Loss=1.6782, Test acc=0.3590\n",
      "Saving...\n",
      "\n",
      "Epoch: 19\n",
      "[Step=3750]\tLoss=1.6659\tacc=0.3801\t2429.2 examples/second\n",
      "[Step=3800]\tLoss=1.6568\tacc=0.3809\t3855.5 examples/second\n",
      "[Step=3850]\tLoss=1.6558\tacc=0.3822\t3922.7 examples/second\n",
      "[Step=3900]\tLoss=1.6497\tacc=0.3842\t3933.2 examples/second\n",
      "Test Loss=1.6613, Test acc=0.3557\n",
      "Files already downloaded and verified\n",
      "Test Loss=1.6782, Test accuracy=0.3590\n",
      "========================\n",
      "Current Nbits: 3\n",
      "Files already downloaded and verified\n",
      "Test Loss=1.2796, Test accuracy=0.5773\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Epoch: 0\n",
      "[Step=50]\tLoss=1.4540\tacc=0.5415\t3749.1 examples/second\n",
      "[Step=100]\tLoss=1.1671\tacc=0.6198\t3948.8 examples/second\n",
      "[Step=150]\tLoss=0.9998\tacc=0.6704\t3806.9 examples/second\n",
      "Test Loss=0.6084, Test acc=0.7936\n",
      "Saving...\n",
      "\n",
      "Epoch: 1\n",
      "[Step=200]\tLoss=0.4958\tacc=0.8359\t2356.2 examples/second\n",
      "[Step=250]\tLoss=0.4844\tacc=0.8341\t3882.4 examples/second\n",
      "[Step=300]\tLoss=0.4589\tacc=0.8444\t3882.0 examples/second\n",
      "[Step=350]\tLoss=0.4475\tacc=0.8478\t3958.8 examples/second\n",
      "Test Loss=0.5011, Test acc=0.8317\n",
      "Saving...\n",
      "\n",
      "Epoch: 2\n",
      "[Step=400]\tLoss=0.4056\tacc=0.8687\t2377.8 examples/second\n",
      "[Step=450]\tLoss=0.3753\tacc=0.8699\t3854.5 examples/second\n",
      "[Step=500]\tLoss=0.3737\tacc=0.8711\t3802.6 examples/second\n",
      "[Step=550]\tLoss=0.3701\tacc=0.8717\t3877.5 examples/second\n",
      "Test Loss=0.4803, Test acc=0.8410\n",
      "Saving...\n",
      "\n",
      "Epoch: 3\n",
      "[Step=600]\tLoss=0.3341\tacc=0.8890\t2423.8 examples/second\n",
      "[Step=650]\tLoss=0.3324\tacc=0.8864\t3969.7 examples/second\n",
      "[Step=700]\tLoss=0.3343\tacc=0.8844\t3927.1 examples/second\n",
      "[Step=750]\tLoss=0.3320\tacc=0.8852\t3910.7 examples/second\n",
      "Test Loss=0.4496, Test acc=0.8508\n",
      "Saving...\n",
      "\n",
      "Epoch: 4\n",
      "[Step=800]\tLoss=0.3016\tacc=0.8972\t2370.9 examples/second\n",
      "[Step=850]\tLoss=0.3083\tacc=0.8925\t3943.5 examples/second\n",
      "[Step=900]\tLoss=0.3101\tacc=0.8922\t3795.0 examples/second\n",
      "[Step=950]\tLoss=0.3082\tacc=0.8925\t3852.7 examples/second\n",
      "Test Loss=0.4433, Test acc=0.8558\n",
      "Saving...\n",
      "\n",
      "Epoch: 5\n",
      "[Step=1000]\tLoss=0.2893\tacc=0.9004\t2478.8 examples/second\n",
      "[Step=1050]\tLoss=0.3000\tacc=0.8937\t3692.2 examples/second\n",
      "[Step=1100]\tLoss=0.2982\tacc=0.8942\t3836.6 examples/second\n",
      "[Step=1150]\tLoss=0.2937\tacc=0.8965\t3976.3 examples/second\n",
      "Test Loss=0.4253, Test acc=0.8605\n",
      "Saving...\n",
      "\n",
      "Epoch: 6\n",
      "[Step=1200]\tLoss=0.2937\tacc=0.9002\t2413.8 examples/second\n",
      "[Step=1250]\tLoss=0.2891\tacc=0.8991\t3832.2 examples/second\n",
      "[Step=1300]\tLoss=0.2808\tacc=0.9019\t3898.7 examples/second\n",
      "[Step=1350]\tLoss=0.2792\tacc=0.9027\t3815.7 examples/second\n",
      "Test Loss=0.4204, Test acc=0.8638\n",
      "Saving...\n",
      "\n",
      "Epoch: 7\n",
      "[Step=1400]\tLoss=0.2717\tacc=0.9040\t2473.9 examples/second\n",
      "[Step=1450]\tLoss=0.2616\tacc=0.9084\t3928.3 examples/second\n",
      "[Step=1500]\tLoss=0.2621\tacc=0.9082\t3766.6 examples/second\n",
      "[Step=1550]\tLoss=0.2634\tacc=0.9082\t3897.1 examples/second\n",
      "Test Loss=0.4042, Test acc=0.8687\n",
      "Saving...\n",
      "\n",
      "Epoch: 8\n",
      "[Step=1600]\tLoss=0.2427\tacc=0.9148\t2398.3 examples/second\n",
      "[Step=1650]\tLoss=0.2507\tacc=0.9128\t3946.1 examples/second\n",
      "[Step=1700]\tLoss=0.2515\tacc=0.9119\t3962.8 examples/second\n",
      "[Step=1750]\tLoss=0.2530\tacc=0.9111\t3773.4 examples/second\n",
      "Test Loss=0.4043, Test acc=0.8711\n",
      "Saving...\n",
      "\n",
      "Epoch: 9\n",
      "[Step=1800]\tLoss=0.2371\tacc=0.9148\t2407.3 examples/second\n",
      "[Step=1850]\tLoss=0.2364\tacc=0.9165\t3955.8 examples/second\n",
      "[Step=1900]\tLoss=0.2417\tacc=0.9145\t3888.1 examples/second\n",
      "[Step=1950]\tLoss=0.2452\tacc=0.9134\t3971.9 examples/second\n",
      "Test Loss=0.4004, Test acc=0.8719\n",
      "Saving...\n",
      "\n",
      "Epoch: 10\n",
      "[Step=2000]\tLoss=0.2440\tacc=0.9149\t2280.5 examples/second\n",
      "[Step=2050]\tLoss=0.2391\tacc=0.9173\t3936.9 examples/second\n",
      "[Step=2100]\tLoss=0.2403\tacc=0.9162\t3886.7 examples/second\n",
      "[Step=2150]\tLoss=0.2365\tacc=0.9173\t3855.9 examples/second\n",
      "Test Loss=0.4029, Test acc=0.8726\n",
      "Saving...\n",
      "\n",
      "Epoch: 11\n",
      "[Step=2200]\tLoss=0.2285\tacc=0.9227\t2321.4 examples/second\n",
      "[Step=2250]\tLoss=0.2291\tacc=0.9215\t3919.3 examples/second\n",
      "[Step=2300]\tLoss=0.2267\tacc=0.9212\t3890.0 examples/second\n",
      "[Step=2350]\tLoss=0.2260\tacc=0.9207\t3936.9 examples/second\n",
      "Test Loss=0.3870, Test acc=0.8745\n",
      "Saving...\n",
      "\n",
      "Epoch: 12\n",
      "[Step=2400]\tLoss=0.2214\tacc=0.9234\t2358.1 examples/second\n",
      "[Step=2450]\tLoss=0.2213\tacc=0.9245\t3805.1 examples/second\n",
      "[Step=2500]\tLoss=0.2215\tacc=0.9234\t3797.8 examples/second\n",
      "Test Loss=0.3792, Test acc=0.8783\n",
      "Saving...\n",
      "\n",
      "Epoch: 13\n",
      "[Step=2550]\tLoss=0.2621\tacc=0.9199\t2433.4 examples/second\n",
      "[Step=2600]\tLoss=0.2231\tacc=0.9229\t3823.8 examples/second\n",
      "[Step=2650]\tLoss=0.2195\tacc=0.9230\t3735.6 examples/second\n",
      "[Step=2700]\tLoss=0.2176\tacc=0.9230\t3704.3 examples/second\n",
      "Test Loss=0.3732, Test acc=0.8799\n",
      "Saving...\n",
      "\n",
      "Epoch: 14\n",
      "[Step=2750]\tLoss=0.1971\tacc=0.9284\t2334.3 examples/second\n",
      "[Step=2800]\tLoss=0.2050\tacc=0.9263\t3950.6 examples/second\n",
      "[Step=2850]\tLoss=0.2052\tacc=0.9277\t3964.7 examples/second\n",
      "[Step=2900]\tLoss=0.2082\tacc=0.9268\t3875.5 examples/second\n",
      "Test Loss=0.3853, Test acc=0.8778\n",
      "\n",
      "Epoch: 15\n",
      "[Step=2950]\tLoss=0.2085\tacc=0.9195\t2421.1 examples/second\n",
      "[Step=3000]\tLoss=0.2058\tacc=0.9255\t3940.9 examples/second\n",
      "[Step=3050]\tLoss=0.2083\tacc=0.9254\t3953.5 examples/second\n",
      "[Step=3100]\tLoss=0.2083\tacc=0.9260\t3955.9 examples/second\n",
      "Test Loss=0.3978, Test acc=0.8770\n",
      "\n",
      "Epoch: 16\n",
      "[Step=3150]\tLoss=0.2197\tacc=0.9213\t2448.6 examples/second\n",
      "[Step=3200]\tLoss=0.2030\tacc=0.9288\t3771.6 examples/second\n",
      "[Step=3250]\tLoss=0.2044\tacc=0.9283\t3804.0 examples/second\n",
      "[Step=3300]\tLoss=0.2043\tacc=0.9286\t3950.8 examples/second\n",
      "Test Loss=0.3777, Test acc=0.8793\n",
      "\n",
      "Epoch: 17\n",
      "[Step=3350]\tLoss=0.2131\tacc=0.9260\t2448.4 examples/second\n",
      "[Step=3400]\tLoss=0.2043\tacc=0.9296\t3928.0 examples/second\n",
      "[Step=3450]\tLoss=0.2059\tacc=0.9281\t3706.9 examples/second\n",
      "[Step=3500]\tLoss=0.2010\tacc=0.9302\t3695.3 examples/second\n",
      "Test Loss=0.3788, Test acc=0.8796\n",
      "\n",
      "Epoch: 18\n",
      "[Step=3550]\tLoss=0.1896\tacc=0.9339\t2422.1 examples/second\n",
      "[Step=3600]\tLoss=0.2027\tacc=0.9308\t3923.0 examples/second\n",
      "[Step=3650]\tLoss=0.1975\tacc=0.9317\t3939.8 examples/second\n",
      "[Step=3700]\tLoss=0.1946\tacc=0.9325\t3904.5 examples/second\n",
      "Test Loss=0.3836, Test acc=0.8795\n",
      "\n",
      "Epoch: 19\n",
      "[Step=3750]\tLoss=0.1951\tacc=0.9312\t2355.9 examples/second\n",
      "[Step=3800]\tLoss=0.1963\tacc=0.9306\t3763.2 examples/second\n",
      "[Step=3850]\tLoss=0.1940\tacc=0.9313\t3855.5 examples/second\n",
      "[Step=3900]\tLoss=0.1932\tacc=0.9313\t3865.3 examples/second\n",
      "Test Loss=0.3702, Test acc=0.8839\n",
      "Saving...\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.3702, Test accuracy=0.8839\n",
      "========================\n",
      "Current Nbits: 4\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.3980, Test accuracy=0.8783\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "Epoch: 0\n",
      "[Step=50]\tLoss=0.6169\tacc=0.8379\t3637.7 examples/second\n",
      "[Step=100]\tLoss=0.4759\tacc=0.8639\t3931.6 examples/second\n",
      "[Step=150]\tLoss=0.4050\tacc=0.8810\t3967.8 examples/second\n",
      "Test Loss=0.4095, Test acc=0.8789\n",
      "Saving...\n",
      "\n",
      "Epoch: 1\n",
      "[Step=200]\tLoss=0.2349\tacc=0.9189\t2452.5 examples/second\n",
      "[Step=250]\tLoss=0.2216\tacc=0.9259\t3956.2 examples/second\n",
      "[Step=300]\tLoss=0.2246\tacc=0.9269\t3906.1 examples/second\n",
      "[Step=350]\tLoss=0.2258\tacc=0.9258\t3939.2 examples/second\n",
      "Test Loss=0.3603, Test acc=0.8862\n",
      "Saving...\n",
      "\n",
      "Epoch: 2\n",
      "[Step=400]\tLoss=0.1845\tacc=0.9448\t2459.7 examples/second\n",
      "[Step=450]\tLoss=0.2100\tacc=0.9331\t3897.7 examples/second\n",
      "[Step=500]\tLoss=0.2030\tacc=0.9340\t3940.1 examples/second\n",
      "[Step=550]\tLoss=0.2033\tacc=0.9332\t3968.2 examples/second\n",
      "Test Loss=0.3408, Test acc=0.8869\n",
      "Saving...\n",
      "\n",
      "Epoch: 3\n",
      "[Step=600]\tLoss=0.1801\tacc=0.9404\t2440.7 examples/second\n",
      "[Step=650]\tLoss=0.1894\tacc=0.9344\t3907.9 examples/second\n",
      "[Step=700]\tLoss=0.1852\tacc=0.9355\t3929.0 examples/second\n",
      "[Step=750]\tLoss=0.1830\tacc=0.9366\t3891.3 examples/second\n",
      "Test Loss=0.3570, Test acc=0.8904\n",
      "Saving...\n",
      "\n",
      "Epoch: 4\n",
      "[Step=800]\tLoss=0.1680\tacc=0.9426\t2404.2 examples/second\n",
      "[Step=850]\tLoss=0.1689\tacc=0.9412\t3851.6 examples/second\n",
      "[Step=900]\tLoss=0.1637\tacc=0.9428\t3929.7 examples/second\n",
      "[Step=950]\tLoss=0.1612\tacc=0.9435\t3923.5 examples/second\n",
      "Test Loss=0.3328, Test acc=0.8938\n",
      "Saving...\n",
      "\n",
      "Epoch: 5\n",
      "[Step=1000]\tLoss=0.1484\tacc=0.9439\t2343.7 examples/second\n",
      "[Step=1050]\tLoss=0.1501\tacc=0.9455\t3938.4 examples/second\n",
      "[Step=1100]\tLoss=0.1475\tacc=0.9472\t3919.8 examples/second\n",
      "[Step=1150]\tLoss=0.1479\tacc=0.9470\t3848.3 examples/second\n",
      "Test Loss=0.3319, Test acc=0.8955\n",
      "Saving...\n",
      "\n",
      "Epoch: 6\n",
      "[Step=1200]\tLoss=0.1408\tacc=0.9502\t2327.5 examples/second\n",
      "[Step=1250]\tLoss=0.1377\tacc=0.9507\t3914.5 examples/second\n",
      "[Step=1300]\tLoss=0.1437\tacc=0.9496\t3955.8 examples/second\n",
      "[Step=1350]\tLoss=0.1423\tacc=0.9498\t3910.3 examples/second\n",
      "Test Loss=0.3341, Test acc=0.8961\n",
      "Saving...\n",
      "\n",
      "Epoch: 7\n",
      "[Step=1400]\tLoss=0.1247\tacc=0.9573\t2384.0 examples/second\n",
      "[Step=1450]\tLoss=0.1319\tacc=0.9542\t3841.2 examples/second\n",
      "[Step=1500]\tLoss=0.1321\tacc=0.9535\t3976.3 examples/second\n",
      "[Step=1550]\tLoss=0.1340\tacc=0.9524\t3887.1 examples/second\n",
      "Test Loss=0.3645, Test acc=0.8940\n",
      "\n",
      "Epoch: 8\n",
      "[Step=1600]\tLoss=0.1314\tacc=0.9524\t2482.6 examples/second\n",
      "[Step=1650]\tLoss=0.1289\tacc=0.9539\t4004.7 examples/second\n",
      "[Step=1700]\tLoss=0.1289\tacc=0.9546\t3938.4 examples/second\n",
      "[Step=1750]\tLoss=0.1272\tacc=0.9550\t3970.5 examples/second\n",
      "Test Loss=0.3553, Test acc=0.8999\n",
      "Saving...\n",
      "\n",
      "Epoch: 9\n",
      "[Step=1800]\tLoss=0.1202\tacc=0.9584\t2368.9 examples/second\n",
      "[Step=1850]\tLoss=0.1230\tacc=0.9568\t3767.1 examples/second\n",
      "[Step=1900]\tLoss=0.1256\tacc=0.9558\t3861.5 examples/second\n",
      "[Step=1950]\tLoss=0.1258\tacc=0.9563\t3977.7 examples/second\n",
      "Test Loss=0.3315, Test acc=0.9033\n",
      "Saving...\n",
      "\n",
      "Epoch: 10\n",
      "[Step=2000]\tLoss=0.1219\tacc=0.9585\t2372.6 examples/second\n",
      "[Step=2050]\tLoss=0.1240\tacc=0.9576\t3610.2 examples/second\n",
      "[Step=2100]\tLoss=0.1235\tacc=0.9572\t3944.7 examples/second\n",
      "[Step=2150]\tLoss=0.1210\tacc=0.9581\t3638.9 examples/second\n",
      "Test Loss=0.3298, Test acc=0.9000\n",
      "\n",
      "Epoch: 11\n",
      "[Step=2200]\tLoss=0.1137\tacc=0.9608\t2454.1 examples/second\n",
      "[Step=2250]\tLoss=0.1191\tacc=0.9580\t3969.8 examples/second\n",
      "[Step=2300]\tLoss=0.1190\tacc=0.9584\t3960.5 examples/second\n",
      "[Step=2350]\tLoss=0.1199\tacc=0.9580\t3832.7 examples/second\n",
      "Test Loss=0.3337, Test acc=0.9004\n",
      "\n",
      "Epoch: 12\n",
      "[Step=2400]\tLoss=0.1119\tacc=0.9621\t2448.7 examples/second\n",
      "[Step=2450]\tLoss=0.1103\tacc=0.9618\t3830.1 examples/second\n",
      "[Step=2500]\tLoss=0.1126\tacc=0.9608\t3762.8 examples/second\n",
      "Test Loss=0.3413, Test acc=0.9013\n",
      "\n",
      "Epoch: 13\n",
      "[Step=2550]\tLoss=0.1039\tacc=0.9648\t2415.7 examples/second\n",
      "[Step=2600]\tLoss=0.1149\tacc=0.9596\t3883.8 examples/second\n",
      "[Step=2650]\tLoss=0.1162\tacc=0.9593\t3941.0 examples/second\n",
      "[Step=2700]\tLoss=0.1165\tacc=0.9594\t3914.1 examples/second\n",
      "Test Loss=0.3405, Test acc=0.8993\n",
      "\n",
      "Epoch: 14\n",
      "[Step=2750]\tLoss=0.1110\tacc=0.9616\t2444.7 examples/second\n",
      "[Step=2800]\tLoss=0.1155\tacc=0.9602\t3911.1 examples/second\n",
      "[Step=2850]\tLoss=0.1178\tacc=0.9598\t3940.7 examples/second\n",
      "[Step=2900]\tLoss=0.1162\tacc=0.9600\t3939.3 examples/second\n",
      "Test Loss=0.3221, Test acc=0.9009\n",
      "\n",
      "Epoch: 15\n",
      "[Step=2950]\tLoss=0.1005\tacc=0.9668\t2477.7 examples/second\n",
      "[Step=3000]\tLoss=0.1058\tacc=0.9620\t3792.1 examples/second\n",
      "[Step=3050]\tLoss=0.1077\tacc=0.9620\t3852.5 examples/second\n",
      "[Step=3100]\tLoss=0.1084\tacc=0.9620\t3678.9 examples/second\n",
      "Test Loss=0.3281, Test acc=0.9001\n",
      "\n",
      "Epoch: 16\n",
      "[Step=3150]\tLoss=0.1160\tacc=0.9604\t2388.1 examples/second\n",
      "[Step=3200]\tLoss=0.1072\tacc=0.9617\t3894.4 examples/second\n",
      "[Step=3250]\tLoss=0.1096\tacc=0.9607\t3887.9 examples/second\n",
      "[Step=3300]\tLoss=0.1105\tacc=0.9606\t3845.8 examples/second\n",
      "Test Loss=0.3336, Test acc=0.9031\n",
      "\n",
      "Epoch: 17\n",
      "[Step=3350]\tLoss=0.1041\tacc=0.9622\t2483.2 examples/second\n",
      "[Step=3400]\tLoss=0.1104\tacc=0.9612\t3889.6 examples/second\n",
      "[Step=3450]\tLoss=0.1054\tacc=0.9635\t3948.8 examples/second\n",
      "[Step=3500]\tLoss=0.1063\tacc=0.9626\t3872.1 examples/second\n",
      "Test Loss=0.3440, Test acc=0.9010\n",
      "\n",
      "Epoch: 18\n",
      "[Step=3550]\tLoss=0.1196\tacc=0.9558\t2419.5 examples/second\n",
      "[Step=3600]\tLoss=0.1075\tacc=0.9618\t3929.9 examples/second\n",
      "[Step=3650]\tLoss=0.1086\tacc=0.9612\t3944.7 examples/second\n",
      "[Step=3700]\tLoss=0.1069\tacc=0.9618\t3970.8 examples/second\n",
      "Test Loss=0.3507, Test acc=0.9005\n",
      "\n",
      "Epoch: 19\n",
      "[Step=3750]\tLoss=0.1136\tacc=0.9581\t2414.5 examples/second\n",
      "[Step=3800]\tLoss=0.1078\tacc=0.9609\t3798.4 examples/second\n",
      "[Step=3850]\tLoss=0.1072\tacc=0.9616\t3873.3 examples/second\n",
      "[Step=3900]\tLoss=0.1089\tacc=0.9611\t3983.4 examples/second\n",
      "Test Loss=0.3473, Test acc=0.8972\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.3315, Test accuracy=0.9033\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "for i in [2,3,4]:\n",
    "    print(f\"Current Nbits: {i}\")\n",
    "    net = ResNetCIFAR(num_layers=20, Nbits=i)\n",
    "    net = net.to(device)\n",
    "    net.load_state_dict(torch.load(\"net_after_global_iterative_prune.pt\"))\n",
    "    test(net)\n",
    "\n",
    "    # Quantized model finetuning\n",
    "    finetune(net, epochs=20, batch_size=256, lr=0.002, reg=1e-4)\n",
    "\n",
    "    # Load the model with best accuracy\n",
    "    net.load_state_dict(torch.load(\"quantized_net_after_finetune.pt\"))\n",
    "    test(net)\n",
    "    print('========================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab3 (e) Symmetric quantization\n",
    "#### Implement symmetric quantization in FP_layers.py, and repeat the process in (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Nbits: 2\n",
      "Files already downloaded and verified\n",
      "Test Loss=42.7781, Test accuracy=0.1000\n",
      "========================\n",
      "Current Nbits: 3\n",
      "Files already downloaded and verified\n",
      "Test Loss=2.3739, Test accuracy=0.5185\n",
      "========================\n",
      "Current Nbits: 4\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.4227, Test accuracy=0.8875\n",
      "========================\n",
      "Current Nbits: 5\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.3520, Test accuracy=0.9083\n",
      "========================\n",
      "Current Nbits: 6\n",
      "Files already downloaded and verified\n",
      "Test Loss=0.3276, Test accuracy=0.9124\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "# check the performance of symmetric quantization with 6, 5, 4, 3, 2 bits\n",
    "for i in [2, 3, 4, 5, 6]:\n",
    "    print(f\"Current Nbits: {i}\")\n",
    "    net = ResNetCIFAR(num_layers=20, Nbits=i, symmetric=True)\n",
    "    net = net.to(device)\n",
    "    net.load_state_dict(torch.load(\"pretrained_model.pt\"))\n",
    "    test(net)\n",
    "\n",
    "    print('========================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
